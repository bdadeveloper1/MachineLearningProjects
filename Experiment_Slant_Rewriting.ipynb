{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdadeveloper1/MachineLearningProjects/blob/main/Experiment_Slant_Rewriting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDA3gUriwi0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdinOR9Viwi5"
      },
      "source": [
        "<h3>Part One: Generate Summaries</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWP03Svaiwi6",
        "outputId": "6390824c-04ba-4413-c15c-77fbf5c4171f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.20.0.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 982 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.62-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.20.0-py3-none-any.whl size=54118 sha256=b4a284f11c65dd09ccb7d772500a5538df70d784f74925f124efe43e34ebf0a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/8d/9b/e28529ec53123e0279208f99148d4661232120d78cb866839b\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, openai\n",
            "Successfully installed openai-0.20.0 pandas-stubs-1.2.0.62\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJMI7GxZiwi7"
      },
      "outputs": [],
      "source": [
        "# This function provides a dictionary with context for GPT-3 in generating its outputs\n",
        "\n",
        "def set_params(topic, position, slant, context=False):\n",
        "    params = {}\n",
        "    \n",
        "    # The topic should be described briefly, in five words or less (e.g. \"Green New Deal\")\n",
        "    params['topic'] = topic\n",
        "    \n",
        "    # The context is optional and should be no longer than a phrase (e.g. \"a resolution recently introduced to Congress\")\n",
        "    params['context'] = context\n",
        "    \n",
        "    # Position: should the new article \"support\" or \"oppose\" the topic? \n",
        "    params['position'] = position\n",
        "    \n",
        "    # Slant: what should the slant of the new article be? (e.g. \"conservative\", \"anti-China\", \"pro-Bernie\")\n",
        "    params['slant'] = slant\n",
        "    \n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rGoJBdUiwi9"
      },
      "outputs": [],
      "source": [
        "# This function generates the prompt for GPT-3 to summarize the article\n",
        "\n",
        "def step_one_prompt(params, article):\n",
        "    if params['context'] == False:\n",
        "        prompt_string = \"Below is a short article about {}. \".format(params['topic'])\n",
        "    else:\n",
        "        prompt_string = \"Below is a short article about {}{}. \".format(params['topic'], params['context'])\n",
        "        \n",
        "    prompt_string += \"After reading it, summarize five key takeaways about {}.\\n\\nText: \".format(params['topic'])\n",
        "    prompt_string += article\n",
        "    prompt_string += \"\"\"\\n\\nNow summarize in short bullet points five key takeaways from this passage. These should be short bullet points under 10 words and they should not directly repeat the original text.\\n1:\"\"\"\n",
        "    \n",
        "    return prompt_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlPj_mbfiwi9"
      },
      "outputs": [],
      "source": [
        "# This function gets five summaries from GPT-3 and returns them in list format\n",
        "\n",
        "def summarize_article(params, article, n=5, temp=0.7):\n",
        "    input_string = step_one_prompt(params, article)\n",
        "    response_full = openai.Completion.create(engine='davinci-instruct-beta', \n",
        "                                             prompt=input_string, \n",
        "                                             max_tokens=400, \n",
        "                                             n=n, \n",
        "                                             temperature=temp, \n",
        "                                             frequency_penalty=0.2)\n",
        "    responses = [response_full.get('choices')[i].text.strip() for i in range(n)]\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH1BjUK_iwi_",
        "outputId": "7c55e603-85cc-46a1-d5fc-6d12a7f23bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is a short article about the Green New Deal, a resolution recently introduced to Congress. After reading it, summarize five key takeaways about the Green New Deal.\n",
            "\n",
            "Text: On February 7, Senator Ed Markey (D-MA) and Congresswoman Alexandria Ocasio-Cortez (D-NY) introduced a Green New Deal resolution to Congress that spells out a transformative path forward on the road to decarbonization. The plan provides an ambitious roadmap to not only decarbonize the economy but also make it fairer, by using taxes on the rich to fund a massive push for renewable energy that doubles as an ambitious jobs-creation program. Other parts of the resolution call for re-investing in American infrastructure and taking steps to ensure that the most vulnerable populations can be adequately protected from the costs of a changing climate. Whether or not it passes, the Green New Deal will undoubtedly come to define the climate goals of the progressive movement for the coming years.\n",
            "\n",
            "Now summarize in short bullet points five key takeaways from this passage. These should be short bullet points under 10 words and they should not directly repeat the original text.\n",
            "1:\n"
          ]
        }
      ],
      "source": [
        "# Example prompt string (without GPT-3 call)\n",
        "\n",
        "params = set_params('the Green New Deal', 'oppose', 'strongly conservative', context=', a resolution recently introduced to Congress')\n",
        "\n",
        "article = \"On February 7, Senator Ed Markey (D-MA) and Congresswoman Alexandria Ocasio-Cortez (D-NY) introduced a Green New Deal resolution to Congress that spells out a transformative path forward on the road to decarbonization. The plan provides an ambitious roadmap to not only decarbonize the economy but also make it fairer, by using taxes on the rich to fund a massive push for renewable energy that doubles as an ambitious jobs-creation program. Other parts of the resolution call for re-investing in American infrastructure and taking steps to ensure that the most vulnerable populations can be adequately protected from the costs of a changing climate. Whether or not it passes, the Green New Deal will undoubtedly come to define the climate goals of the progressive movement for the coming years.\"\n",
        "\n",
        "print(step_one_prompt(params, article))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyS2KAB2iwjB"
      },
      "source": [
        "<h3>Part Two: Quality Checks for Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWAlonrjiwjB"
      },
      "outputs": [],
      "source": [
        "# This function creates a df of the five generated summaries with \n",
        "# fields for the relevant measures we'll use to evaluate them\n",
        "\n",
        "def init_response_df(responses):\n",
        "    df = pd.DataFrame(responses, columns=['text'])\n",
        "    \n",
        "    n = len(responses)\n",
        "    \n",
        "    df['sentences'] = [[]] * n\n",
        "    df['tokens'] = [[]] * n\n",
        "    df['format_check'] = [False] * n\n",
        "    df['repetition_check'] = [False] * n\n",
        "    df['repetition_score'] = [1] * n\n",
        "    df['avg_length'] = [100] * n\n",
        "    df['quality_score'] = [0] * n\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfudDjHHiwjB"
      },
      "outputs": [],
      "source": [
        "# This function splits the full text output for each summary into a list of its bullet points\n",
        "# It then checks that the GPT-3 output contained five distinct bullets\n",
        "\n",
        "def check_format(df):\n",
        "    responses = df.text.tolist()\n",
        "    responses_split = [re.split('\\n\\d: ', response) for response in responses]\n",
        "    \n",
        "    df['sentences'] = responses_split\n",
        "    \n",
        "    format_check = [True if len(r) == 5 else False for r in responses_split]\n",
        "    df['format_check'] = format_check\n",
        "    \n",
        "    if sum(format_check) == 0:\n",
        "        print('Quality check failed: no outputs formatted correctly.')\n",
        "    else:\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPmxD8bfiwjC"
      },
      "outputs": [],
      "source": [
        "# This function checks that not only does a GPT-3 output contain five bullet points, but none of those\n",
        "# bullet points are an exact duplicate of another (a fairly common problem)\n",
        "\n",
        "def remove_repeats(df):\n",
        "    responses = df.sentences.tolist()\n",
        "    repetition_check = [True if len(list(set(r))) == 5 else False for r in responses]\n",
        "    df['repetition_check'] = repetition_check\n",
        "    \n",
        "    if sum(repetition_check) == 0:\n",
        "        print('Quality check failed: all outputs contained repetition.')\n",
        "    else:\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-k8MJtiyiwjC",
        "outputId": "77c92421-6b24-479c-9934-cb4cdda33e72"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7720a42d444f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The result is a list of lists for each summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# This function breaks the summaries into tokens and lemmatizes them. \n",
        "# The result is a list of lists for each summary \n",
        "\n",
        "stop = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize(df):\n",
        "    tokens = []\n",
        "    outputs = df.sentences.tolist()\n",
        "    \n",
        "    for sentences in outputs:\n",
        "        output_toks = []\n",
        "        for sent in sentences:\n",
        "            \n",
        "            # Remove numbers and punctuation\n",
        "            clean = re.sub('\\d+|\\W+', ' ', sent.lower())\n",
        "            \n",
        "            # Split into words on whitespace characters\n",
        "            toks = re.split('\\s+', clean.strip())\n",
        "            \n",
        "            # Remove stop words\n",
        "            toks = [tok for tok in toks if tok not in stop]\n",
        "            \n",
        "            # Lemmatize\n",
        "            lemmas = [lemmatizer.lemmatize(tok) for tok in toks]\n",
        "            output_toks.append(lemmas)\n",
        "            \n",
        "        tokens.append(output_toks)\n",
        "    df['tokens'] = tokens\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEk4VI-MiwjC"
      },
      "outputs": [],
      "source": [
        "# This function creates a repetition score by measuring how much the five bullet points overlap with each other\n",
        "\n",
        "def calc_repetition_score(df):\n",
        "    outputs = df.tokens.tolist()\n",
        "    \n",
        "    repetition_scores = []\n",
        "    \n",
        "    for output in outputs:\n",
        "        \n",
        "        # Create zero-matrix of size (5,5) (assuming output correctly contains 5 bullet points)\n",
        "        sims = np.zeros(shape=(len(output), len(output)))\n",
        "        \n",
        "        for i in range(len(output)):\n",
        "            for j in range(len(output)):\n",
        "                \n",
        "                # Calculate pairwise comparisons as number of tokens that show up in both bullet points\n",
        "                # divided by the total number of tokens in the shorter bullet point\n",
        "                num_overlaps = len(list(set(output[i]) & set(output[j])))\n",
        "                denom = min([len(set(output[i])), len(set(output[j]))])\n",
        "                sims[i][j] = num_overlaps / denom\n",
        "        \n",
        "        # Remove diagonal\n",
        "        score_by_col = (sims.sum(1)-np.diag(sims))/(sims.shape[1]-1)\n",
        "        \n",
        "        # Take the mean of all pairwise comparisons as the overall repetition score\n",
        "        score = np.mean(score_by_col)\n",
        "        repetition_scores.append(score)\n",
        "        \n",
        "    df['repetition_score'] = repetition_scores\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQia2F8YiwjC"
      },
      "outputs": [],
      "source": [
        "# This function calculates the average length (in non-stopword tokens) of each bullet point\n",
        "\n",
        "def calc_avg_length(df):\n",
        "    avg_lengths = []\n",
        "    outputs = df.tokens.tolist()\n",
        "    \n",
        "    for output in outputs:\n",
        "        avg = np.mean([len(sent) for sent in output])\n",
        "        avg_lengths.append(avg)\n",
        "        \n",
        "    df['avg_length'] = avg_lengths\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz0AnIfYiwjC"
      },
      "outputs": [],
      "source": [
        "# This function calculates an overall quality score using the repetition_score and avg_length fields\n",
        "\n",
        "def calc_quality_score(df):\n",
        "    quality_scores = []\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        # Assign score of 0 if output does not consist of five unique bullet points\n",
        "        if df['format_check'][i] == False:\n",
        "            quality_scores.append(0)\n",
        "        elif df['repetition_check'][i] == False:\n",
        "            quality_scores.append(0)\n",
        "            \n",
        "        # Otherwise, calculate score as 100 - 100*the repetition score - the mean distance of sentence length from 7 tokens\n",
        "        else:\n",
        "            dist_from_seven = np.abs(7 - df['avg_length'][i])\n",
        "            quality_scores.append(100 - df['repetition_score'][i]*100 - dist_from_seven*5)\n",
        "    \n",
        "    df['quality_score'] = quality_scores\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYxYV0IYiwjD"
      },
      "outputs": [],
      "source": [
        "# This function uses the previous functions to pick the best summary and returns the text and score\n",
        "\n",
        "def pick_best_summary(summaries):\n",
        "    df = init_response_df(summaries)\n",
        "    df = check_format(df)\n",
        "    df = remove_repeats(df)\n",
        "    df = tokenize(df)\n",
        "    df = calc_repetition_score(df)\n",
        "    df = calc_avg_length(df)\n",
        "    df = calc_quality_score(df)\n",
        "    \n",
        "    max_row = df[df.quality_score == df.quality_score.max()].reset_index()\n",
        "    return max_row.text[0], df.quality_score.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdP0RNSiwjD"
      },
      "source": [
        "<h3>Part Three: Using the Summary to Rewrite the Article from a New Slant</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct2vM4EhiwjD"
      },
      "outputs": [],
      "source": [
        "# This function uses the same dictionary of context to generate a prompt for GPT-3\n",
        "\n",
        "def step_two_prompt(params, summary):\n",
        "    if params['context'] == False:\n",
        "        prompt_string = \"Below is a short list of facts regarding {}:\".format(params['topic'])\n",
        "    else:\n",
        "        prompt_string = \"Below is a short list of facts regarding {}{}:\".format(params['topic'], params['context'])\n",
        "        \n",
        "    prompt_string += \"\\n\\n1: \"\n",
        "    prompt_string += summary\n",
        "    prompt_string += \"\\n\\nUse this set of facts to write a headline and accompanying article that {}s {} from a strongly {} slant.\".format(params['position'], params['topic'], params['slant'])\n",
        "    prompt_string += \"\\n\\nHeadline:\"\n",
        "    \n",
        "    return prompt_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stUmcZCpiwjD"
      },
      "outputs": [],
      "source": [
        "# This function calls GPT-3 to generate a rewrite\n",
        "\n",
        "def rewrite_from_summary(params, summary, n_outputs=1, temp=0.7):\n",
        "    input_string = step_two_prompt(params, summary)\n",
        "    response_full = openai.Completion.create(engine='davinci-instruct-beta',\n",
        "                                            prompt=input_string,\n",
        "                                            max_tokens=200,\n",
        "                                            n=n_outputs,\n",
        "                                            temperature=temp,\n",
        "                                            frequency_penalty=0.2)\n",
        "    responses = [response_full.get('choices')[i].text.strip() for i in range(n_outputs)]\n",
        "    return responses\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS22rg_HiwjE"
      },
      "outputs": [],
      "source": [
        "# This function also performs a quality check on the rewrite\n",
        "\n",
        "# It contains much of the same functionality as the previous quality checks, but in more compressed form\n",
        "\n",
        "def quality_check_rewrite(rewrites):\n",
        "    repetition_scores = []\n",
        "    \n",
        "    for rewrite in rewrites:\n",
        "        \n",
        "        # Remove numbers and punctuation\n",
        "        rewrite_clean = re.sub('\\d+|W+', ' ', rewrite)\n",
        "        \n",
        "        # Tokenize on whitespace and remove stopwords\n",
        "        toks = re.split('\\s+', rewrite_clean)\n",
        "        toks = [t for t in toks if t not in stop]\n",
        "        \n",
        "        # If the length of the output is over 75 tokens, assign repetition score of number of unique tokens\n",
        "        # divided by total number of tokens; else set repetition score to 1\n",
        "        \n",
        "        # (Sometimes the outputs would cut off suddenly; the length check weeded out those outputs)\n",
        "        if len(toks) > 75:\n",
        "            repetition_scores.append(1 - len(list(set(toks)))/len(toks))\n",
        "        else:\n",
        "            repetition_scores.append(1)\n",
        "    \n",
        "    # Use the repetition score to identify the best rewrite\n",
        "    for r, s in zip(rewrites, repetition_scores):\n",
        "        if s == min(repetition_scores):\n",
        "            best_rewrite = r\n",
        "            \n",
        "    return best_rewrite, min(repetition_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKB4B7LUiwjE"
      },
      "source": [
        "<h3>Part Four: Combining It All</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j57mOk-jiwjE"
      },
      "outputs": [],
      "source": [
        "# This function combines all of the previous steps into one function call\n",
        "\n",
        "def rewrite_from_raw(article, # The text of the article\n",
        "                     topic, # Context field for GPT-3\n",
        "                     position, # The intended position of the rewrite\n",
        "                     slant, # The intended slant of the rewrite\n",
        "                     context=False, # Optional field to provide additional context\n",
        "                     n_summaries=5, # Number of summaries generated at each intermediate call\n",
        "                     n_outputs=1, # Number of rewrites generated at each call\n",
        "                     temperature=0.7):\n",
        "    \n",
        "    # First generate dictionary of context fields\n",
        "    params = set_params(topic, position, slant, context=False)\n",
        "    \n",
        "    # Generate summaries in batches of five until either a quality score over 90 is obtained or 25 summaries have been generated\n",
        "    quality_score = 0\n",
        "    i = 0\n",
        "    while i < 5 and quality_score < 90:\n",
        "        summaries = summarize_article(params, article, n=n_summaries, temp=temperature)\n",
        "        best_summary, quality_score = pick_best_summary(summaries)\n",
        "        i += 1\n",
        "        if quality_score >= 90:\n",
        "            print('Summary obtained at iteration {}'.format(i))\n",
        "        elif i == 5:\n",
        "            print('Failed to obtain quality score > 90; proceeding with best alternative.')\n",
        "\n",
        "            \n",
        "    # Generate rewrites one at a time until either a repetition score under 5 is obtained or 5 rewrites have been generated    \n",
        "    repetition_score = 1\n",
        "    i = 0\n",
        "    while i < 5 and repetition_score > 0.5:\n",
        "        rewrites = rewrite_from_summary(params, best_summary, n_outputs=n_outputs, temp=temperature)\n",
        "        best_rewrite, repetition_score = quality_check_rewrite(rewrites)\n",
        "        i += 1\n",
        "        if repetition_score <= 0.5:\n",
        "            print('Rewrite obtained at iteration {}'.format(i))\n",
        "        elif i == 5:\n",
        "            print('Failed to obtain sufficiently long rewrite; proceeding with best alternative.')\n",
        "    \n",
        "    print('————————')\n",
        "    \n",
        "    return best_rewrite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "n7yhnmBaiwjF",
        "outputId": "5de27fc5-969f-4053-895b-4d59685bf8bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1ce7dfab494c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# For each topic, there will be two rewrites for and two against the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'key.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'key.csv'"
          ]
        }
      ],
      "source": [
        "# Read in a .csv file with the relevant parameters for each generation\n",
        "\n",
        "# For each topic, there will be two rewrites for and two against the topic\n",
        "\n",
        "df = pd.read_csv('key.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIrT9fTBiwjF"
      },
      "outputs": [],
      "source": [
        "# Finally, generate the GPT-3 outputs and append to the df, then save as a .csv\n",
        "\n",
        "gpt3_outputs = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    output = None\n",
        "    output = rewrite_from_raw(article=df['original'][i], \n",
        "                              topic=df['topic'][i], \n",
        "                              position=df['position'][i], \n",
        "                              slant=df['slant'][i], \n",
        "                              context=df['context'][i], \n",
        "                              n_summaries=5,\n",
        "                              n_outputs=5,\n",
        "                              temperature=0.75)\n",
        "    if output:\n",
        "        gpt3_outputs.append(output)\n",
        "    else:\n",
        "        gpt3_outputs.append(None)\n",
        "        \n",
        "df['output'] = gpt3_outputs\n",
        "\n",
        "df.to_csv('rewritings.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Experiment Slant_Rewriting.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
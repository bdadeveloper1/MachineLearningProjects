{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Train-2048.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdadeveloper1/MachineLearningProjects/blob/main/Train_2048.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "tabmchrptzEt"
      },
      "source": [
        "# 2048 Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYtB82sOtzEv"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFxcoFEstzEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9239de89-5e46-4e71-b79e-2a14de66e4a6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import random \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pguC-FiDtzE2"
      },
      "source": [
        "### Game Logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3sUXp0xtzE3"
      },
      "source": [
        "#initialize a new game\n",
        "def new_game(n):\n",
        "    matrix = np.zeros([n,n])\n",
        "    return matrix\n",
        "\n",
        "#add 2 or 4 in the matrix\n",
        "def add_two(mat):\n",
        "    empty_cells = []\n",
        "    for i in range(len(mat)):\n",
        "        for j in range(len(mat[0])):\n",
        "            if(mat[i][j]==0):\n",
        "                empty_cells.append((i,j))\n",
        "    if(len(empty_cells)==0):\n",
        "        return mat\n",
        "    \n",
        "    index_pair = empty_cells[random.randint(0,len(empty_cells)-1)]\n",
        "    \n",
        "    prob = random.random()\n",
        "    if(prob>=0.9):\n",
        "        mat[index_pair[0]][index_pair[1]]=4\n",
        "    else:\n",
        "        mat[index_pair[0]][index_pair[1]]=2\n",
        "    return mat\n",
        "\n",
        "#to check state of the game\n",
        "def game_state(mat):\n",
        "    #if 2048 in mat:\n",
        "    #    return 'win'\n",
        "    \n",
        "    for i in range(len(mat)-1): #intentionally reduced to check the row on the right and below\n",
        "        for j in range(len(mat[0])-1): #more elegant to use exceptions but most likely this will be their solution\n",
        "            if mat[i][j]==mat[i+1][j] or mat[i][j+1]==mat[i][j]:\n",
        "                return 'not over'\n",
        "            \n",
        "    for i in range(len(mat)): #check for any zero entries\n",
        "        for j in range(len(mat[0])):\n",
        "            if mat[i][j]==0:\n",
        "                return 'not over'\n",
        "            \n",
        "    for k in range(len(mat)-1): #to check the left/right entries on the last row\n",
        "        if mat[len(mat)-1][k]==mat[len(mat)-1][k+1]:\n",
        "            return 'not over'\n",
        "        \n",
        "    for j in range(len(mat)-1): #check up/down entries on last column\n",
        "        if mat[j][len(mat)-1]==mat[j+1][len(mat)-1]:\n",
        "            return 'not over'\n",
        "        \n",
        "    return 'lose'\n",
        "\n",
        "\n",
        "def reverse(mat):\n",
        "    new=[]\n",
        "    for i in range(len(mat)):\n",
        "        new.append([])\n",
        "        for j in range(len(mat[0])):\n",
        "            new[i].append(mat[i][len(mat[0])-j-1])\n",
        "    return new\n",
        "\n",
        "def transpose(mat):\n",
        "    new=[]\n",
        "    for i in range(len(mat[0])):\n",
        "        new.append([])\n",
        "        for j in range(len(mat)):\n",
        "            new[i].append(mat[j][i])\n",
        "            \n",
        "    return np.transpose(mat)\n",
        "\n",
        "def cover_up(mat):\n",
        "    new = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "    done = False\n",
        "    for i in range(4):\n",
        "        count = 0\n",
        "        for j in range(4):\n",
        "            if mat[i][j]!=0:\n",
        "                new[i][count] = mat[i][j]\n",
        "                if j!=count:\n",
        "                    done=True\n",
        "                count+=1\n",
        "    return (new,done)\n",
        "\n",
        "def merge(mat):\n",
        "    done=False\n",
        "    score = 0\n",
        "    for i in range(4):\n",
        "        for j in range(3):\n",
        "            if mat[i][j]==mat[i][j+1] and mat[i][j]!=0:\n",
        "                mat[i][j]*=2\n",
        "                score += mat[i][j]   \n",
        "                mat[i][j+1]=0\n",
        "                done=True\n",
        "    return (mat,done,score)\n",
        "\n",
        "#up move\n",
        "def up(game):\n",
        "        game = transpose(game)\n",
        "        game,done = cover_up(game)\n",
        "        temp = merge(game)\n",
        "        game = temp[0]\n",
        "        done = done or temp[1]\n",
        "        game = cover_up(game)[0]\n",
        "        game = transpose(game)\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#down move\n",
        "def down(game):\n",
        "        game=reverse(transpose(game))\n",
        "        game,done=cover_up(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=cover_up(game)[0]\n",
        "        game=transpose(reverse(game))\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#left move\n",
        "def left(game):\n",
        "        game,done=cover_up(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=cover_up(game)[0]\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#right move\n",
        "def right(game):\n",
        "        game=reverse(game)\n",
        "        game,done=cover_up(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=cover_up(game)[0]\n",
        "        game=reverse(game)\n",
        "        return (game,done,temp[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0dJAVltzE9"
      },
      "source": [
        "### Controls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHMGdBOvtzE-"
      },
      "source": [
        "controls = {0:up,1:left,2:right,3:down}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yh-nkaKtzFB"
      },
      "source": [
        "### Important Functions\n",
        "* Find Empty Cell Function (Used in Reward)\n",
        "* Convert Input Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CxXkvcjtzFC"
      },
      "source": [
        "#convert the input game matrix into corresponding power of 2 matrix.\n",
        "def change_values(X):\n",
        "    power_mat = np.zeros(shape=(1,4,4,16),dtype=np.float32)\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            if(X[i][j]==0):\n",
        "                power_mat[0][i][j][0] = 1.0\n",
        "            else:\n",
        "                power = int(math.log(X[i][j],2))\n",
        "                power_mat[0][i][j][power] = 1.0\n",
        "    return power_mat        \n",
        "\n",
        "#find the number of empty cells in the game matrix.\n",
        "def findemptyCell(mat):\n",
        "    count = 0\n",
        "    for i in range(len(mat)):\n",
        "        for j in range(len(mat)):\n",
        "            if(mat[i][j]==0):\n",
        "                count+=1\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp0_wnUMtzFI"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dTPeTDUtzFJ"
      },
      "source": [
        "#hyper parameters\n",
        "start_learning_rate = 0.0005\n",
        "\n",
        "#gamma for Q-learning\n",
        "gamma = 0.9\n",
        "\n",
        "#epsilon greedy approach\n",
        "epsilon = 0.9\n",
        "\n",
        "#to store states and lables of the game for training\n",
        "#states of the game\n",
        "replay_memory = list()\n",
        "\n",
        "#labels of the states\n",
        "replay_labels = list()\n",
        "\n",
        "#capacity of memory\n",
        "mem_capacity = 6000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3mNykBtzFP"
      },
      "source": [
        "### Network Architecture\n",
        "\n",
        "![](https://github.com/navjindervirdee/2048-deep-reinforcement-learning/blob/master/Architecture/Architecture.JPG?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmAjsme8tzFR"
      },
      "source": [
        "#first convolution layer depth\n",
        "depth1 = 128\n",
        "\n",
        "#second convolution layer depth\n",
        "depth2 = 128\n",
        "\n",
        "#batch size for batch gradient descent\n",
        "batch_size = 512\n",
        "\n",
        "#input units\n",
        "input_units = 16\n",
        "\n",
        "#fully connected layer neurons\n",
        "hidden_units = 256\n",
        "\n",
        "#output neurons = number of moves\n",
        "output_units = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqVFN5ZQtzFX"
      },
      "source": [
        "### Let's make the Tensorflow Graph\n",
        "* Loss = mean ( square( Q(st,at) - (r + gamma x max(Q(st+1,a))) ) )\n",
        "* Activation = RELU\n",
        "* Optimizer = RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TGA6GUFtzFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7bfef828-6e89-49fe-8d30-1ee46afe9d39"
      },
      "source": [
        "#input data\n",
        "tf_batch_dataset = tf.placeholder(tf.float32,shape=(batch_size,4,4,16))\n",
        "tf_batch_labels  = tf.placeholder(tf.float32,shape=(batch_size,output_units))\n",
        "\n",
        "single_dataset   = tf.placeholder(tf.float32,shape=(1,4,4,16))\n",
        "\n",
        "\n",
        "#CONV LAYERS\n",
        "#conv layer1 weights\n",
        "conv1_layer1_weights = tf.Variable(tf.truncated_normal([1,2,input_units,depth1],mean=0,stddev=0.01))\n",
        "conv2_layer1_weights = tf.Variable(tf.truncated_normal([2,1,input_units,depth1],mean=0,stddev=0.01))\n",
        "\n",
        "#conv layer2 weights\n",
        "conv1_layer2_weights = tf.Variable(tf.truncated_normal([1,2,depth1,depth2],mean=0,stddev=0.01))\n",
        "conv2_layer2_weights = tf.Variable(tf.truncated_normal([2,1,depth1,depth2],mean=0,stddev=0.01))\n",
        "\n",
        "\n",
        "\n",
        "#FUllY CONNECTED LAYERS\n",
        "expand_size = 2*4*depth2*2 + 3*3*depth2*2 + 4*3*depth1*2\n",
        "fc_layer1_weights = tf.Variable(tf.truncated_normal([expand_size,hidden_units],mean=0,stddev=0.01))\n",
        "fc_layer1_biases = tf.Variable(tf.truncated_normal([1,hidden_units],mean=0,stddev=0.01))\n",
        "fc_layer2_weights = tf.Variable(tf.truncated_normal([hidden_units,output_units],mean=0,stddev=0.01))\n",
        "fc_layer2_biases = tf.Variable(tf.truncated_normal([1,output_units],mean=0,stddev=0.01))\n",
        "\n",
        "\n",
        "#model\n",
        "def model(dataset):\n",
        "    #layer1\n",
        "    conv1 = tf.nn.conv2d(dataset,conv1_layer1_weights,[1,1,1,1],padding='VALID') \n",
        "    conv2 = tf.nn.conv2d(dataset,conv2_layer1_weights,[1,1,1,1],padding='VALID') \n",
        "    \n",
        "    #layer1 relu activation\n",
        "    relu1 = tf.nn.relu(conv1)\n",
        "    relu2 = tf.nn.relu(conv2)\n",
        "    \n",
        "    #layer2\n",
        "    conv11 = tf.nn.conv2d(relu1,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "    conv12 = tf.nn.conv2d(relu1,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "\n",
        "    conv21 = tf.nn.conv2d(relu2,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "    conv22 = tf.nn.conv2d(relu2,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "\n",
        "    #layer2 relu activation\n",
        "    relu11 = tf.nn.relu(conv11)\n",
        "    relu12 = tf.nn.relu(conv12)\n",
        "    relu21 = tf.nn.relu(conv21)\n",
        "    relu22 = tf.nn.relu(conv22)\n",
        "    \n",
        "    #get shapes of all activations\n",
        "    shape1 = relu1.get_shape().as_list()\n",
        "    shape2 = relu2.get_shape().as_list()\n",
        "    \n",
        "    shape11 = relu11.get_shape().as_list()\n",
        "    shape12 = relu12.get_shape().as_list()\n",
        "    shape21 = relu21.get_shape().as_list()\n",
        "    shape22 = relu22.get_shape().as_list()\n",
        "\n",
        "    #expansion\n",
        "    hidden1 = tf.reshape(relu1,[shape1[0],shape1[1]*shape1[2]*shape1[3]])\n",
        "    hidden2 = tf.reshape(relu2,[shape2[0],shape2[1]*shape2[2]*shape2[3]])\n",
        "    \n",
        "    hidden11 = tf.reshape(relu11,[shape11[0],shape11[1]*shape11[2]*shape11[3]])\n",
        "    hidden12 = tf.reshape(relu12,[shape12[0],shape12[1]*shape12[2]*shape12[3]])\n",
        "    hidden21 = tf.reshape(relu21,[shape21[0],shape21[1]*shape21[2]*shape21[3]])\n",
        "    hidden22 = tf.reshape(relu22,[shape22[0],shape22[1]*shape22[2]*shape22[3]])\n",
        "\n",
        "    #concatenation\n",
        "    hidden = tf.concat([hidden1,hidden2,hidden11,hidden12,hidden21,hidden22],axis=1)\n",
        "\n",
        "    #full connected layers\n",
        "    hidden = tf.matmul(hidden,fc_layer1_weights) + fc_layer1_biases\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "\n",
        "    #output layer\n",
        "    output = tf.matmul(hidden,fc_layer2_weights) + fc_layer2_biases\n",
        "    \n",
        "    #return output\n",
        "    return output\n",
        "\n",
        "#for single example\n",
        "single_output = model(single_dataset)\n",
        "\n",
        "#for batch data\n",
        "logits = model(tf_batch_dataset)\n",
        "\n",
        "#loss\n",
        "loss = tf.square(tf.subtract(tf_batch_labels,logits))\n",
        "loss = tf.reduce_sum(loss,axis=1,keep_dims=True)\n",
        "loss = tf.reduce_mean(loss)/2.0\n",
        "\n",
        "#optimizer\n",
        "global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "learning_rate = tf.train.exponential_decay(float(start_learning_rate), global_step, 1000, 0.90, staircase=True)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-ipcrZMtzFd"
      },
      "source": [
        "#loss\n",
        "J = []\n",
        "\n",
        "#scores\n",
        "scores = []\n",
        "\n",
        "#to store final parameters\n",
        "final_parameters = {}\n",
        "\n",
        "#number of episodes\n",
        "M = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsK0EYfYtzFh"
      },
      "source": [
        "### Create training dataset and Train Simultaneously\n",
        "* Current Reward = number of merges + log(new max,2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UXHLNxntzFi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9876d92-1a4c-40db-b62b-5642556fa2ca"
      },
      "source": [
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    global epsilon\n",
        "    global replay_labels\n",
        "    global replay_memory\n",
        "\n",
        "    #for episode with max score\n",
        "    maximum = -1\n",
        "    episode = -1\n",
        "    \n",
        "    #total_iters \n",
        "    total_iters = 1\n",
        "    \n",
        "    #number of back props\n",
        "    back=0\n",
        "    \n",
        "    for ep in range(M):\n",
        "        global board\n",
        "        board = new_game(4)\n",
        "        add_two(board)\n",
        "        add_two(board)\n",
        "        \n",
        "        #whether episode finished or not\n",
        "        finish = 'not over'\n",
        "        \n",
        "        #total_score of this episode\n",
        "        total_score = 0\n",
        "        \n",
        "        #iters per episode\n",
        "        local_iters = 1\n",
        "        \n",
        "        while(finish=='not over'):\n",
        "            prev_board = deepcopy(board)\n",
        "            \n",
        "            #get the required move for this state\n",
        "            state = deepcopy(board)\n",
        "            state = change_values(state)\n",
        "            state = np.array(state,dtype = np.float32).reshape(1,4,4,16)\n",
        "            feed_dict = {single_dataset:state}\n",
        "            control_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "            \n",
        "            #find the move with max Q value\n",
        "            control_buttons = np.flip(np.argsort(control_scores),axis=1)\n",
        "            \n",
        "            #copy the Q-values as labels\n",
        "            labels = deepcopy(control_scores[0])\n",
        "            \n",
        "            #generate random number for epsilon greedy approach\n",
        "            num = random.uniform(0,1)\n",
        "            \n",
        "            #store prev max\n",
        "            prev_max = np.max(prev_board)\n",
        "            \n",
        "            #num is less epsilon generate random move\n",
        "            if(num<epsilon):\n",
        "                #find legal moves\n",
        "                legal_moves = list()\n",
        "                for i in range(4):\n",
        "                    temp_board = deepcopy(prev_board)\n",
        "                    temp_board,_,_ = controls[i](temp_board)\n",
        "                    if(np.array_equal(temp_board,prev_board)):\n",
        "                        continue\n",
        "                    else:\n",
        "                        legal_moves.append(i)\n",
        "                if(len(legal_moves)==0):\n",
        "                    finish = 'lose'\n",
        "                    continue\n",
        "                \n",
        "                #generate random move.\n",
        "                con = random.sample(legal_moves,1)[0]\n",
        "                \n",
        "                #apply the move\n",
        "                temp_state = deepcopy(prev_board)\n",
        "                temp_state,_,score = controls[con](temp_state)\n",
        "                total_score += score\n",
        "                finish = game_state(temp_state)\n",
        "                \n",
        "                #get number of merges\n",
        "                empty1 = findemptyCell(prev_board)\n",
        "                empty2 = findemptyCell(temp_state)\n",
        "                \n",
        "                if(finish=='not over'):\n",
        "                    temp_state = add_two(temp_state)\n",
        "\n",
        "                board = deepcopy(temp_state)\n",
        "\n",
        "                #get next max after applying the move\n",
        "                next_max = np.max(temp_state)\n",
        "                \n",
        "                #reward math.log(next_max,2)*0.1 if next_max is higher than prev max\n",
        "                labels[con] = (math.log(next_max,2))*0.1\n",
        "                \n",
        "                if(next_max==prev_max):\n",
        "                    labels[con] = 0\n",
        "                \n",
        "                #reward is also the number of merges\n",
        "                labels[con] += (empty2-empty1)\n",
        "                \n",
        "                #get the next state max Q-value\n",
        "                temp_state = change_values(temp_state)\n",
        "                temp_state = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
        "                feed_dict = {single_dataset:temp_state}\n",
        "                temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "                    \n",
        "                max_qvalue = np.max(temp_scores)\n",
        "                \n",
        "                #final labels add gamma*max_qvalue\n",
        "                labels[con] = (labels[con] + gamma*max_qvalue)\n",
        "            \n",
        "            #generate the the max predicted move\n",
        "            else:\n",
        "                for con in control_buttons[0]:\n",
        "                    prev_state = deepcopy(prev_board)\n",
        "                    \n",
        "                    #apply the LEGAl Move with max q_value\n",
        "                    temp_state,_,score = controls[con](prev_state)\n",
        "                    \n",
        "                    #if illegal move label = 0\n",
        "                    if(np.array_equal(prev_board,temp_state)):\n",
        "                        labels[con] = 0\n",
        "                        continue\n",
        "                        \n",
        "                    #get number of merges\n",
        "                    empty1 = findemptyCell(prev_board)\n",
        "                    empty2 = findemptyCell(temp_state)\n",
        "\n",
        "                    \n",
        "                    temp_state = add_two(temp_state)\n",
        "                    board = deepcopy(temp_state)\n",
        "                    total_score += score\n",
        "\n",
        "                    next_max = np.max(temp_state)\n",
        "                    \n",
        "                    #reward\n",
        "                    labels[con] = (math.log(next_max,2))*0.1\n",
        "                    if(next_max==prev_max):\n",
        "                        labels[con] = 0\n",
        "                    \n",
        "                    labels[con] += (empty2-empty1)\n",
        "\n",
        "                    #get next max qvalue\n",
        "                    temp_state = change_values(temp_state)\n",
        "                    temp_state = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
        "                    feed_dict = {single_dataset:temp_state}\n",
        "                    temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "\n",
        "                    max_qvalue = np.max(temp_scores)\n",
        "\n",
        "                    #final labels\n",
        "                    labels[con] = (labels[con] + gamma*max_qvalue)\n",
        "                    break\n",
        "                    \n",
        "                if(np.array_equal(prev_board,board)):\n",
        "                    finish = 'lose'\n",
        "            \n",
        "            #decrease the epsilon value\n",
        "            if((ep>10000) or (epsilon>0.1 and total_iters%2500==0)):\n",
        "                epsilon = epsilon/1.005\n",
        "                \n",
        "           \n",
        "            #change the matrix values and store them in memory\n",
        "            prev_state = deepcopy(prev_board)\n",
        "            prev_state = change_values(prev_state)\n",
        "            prev_state = np.array(prev_state,dtype=np.float32).reshape(1,4,4,16)\n",
        "            replay_labels.append(labels)\n",
        "            replay_memory.append(prev_state)\n",
        "            \n",
        "            \n",
        "            #back-propagation\n",
        "            if(len(replay_memory)>=mem_capacity):\n",
        "                back_loss = 0\n",
        "                batch_num = 0\n",
        "                z = list(zip(replay_memory,replay_labels))\n",
        "                np.random.shuffle(z)\n",
        "                np.random.shuffle(z)\n",
        "                replay_memory,replay_labels = zip(*z)\n",
        "                \n",
        "                for i in range(0,len(replay_memory),batch_size):\n",
        "                    if(i + batch_size>len(replay_memory)):\n",
        "                        break\n",
        "                        \n",
        "                    batch_data = deepcopy(replay_memory[i:i+batch_size])\n",
        "                    batch_labels = deepcopy(replay_labels[i:i+batch_size])\n",
        "                    \n",
        "                    batch_data = np.array(batch_data,dtype=np.float32).reshape(batch_size,4,4,16)\n",
        "                    batch_labels = np.array(batch_labels,dtype=np.float32).reshape(batch_size,output_units)\n",
        "                \n",
        "                    feed_dict = {tf_batch_dataset: batch_data, tf_batch_labels: batch_labels}\n",
        "                    _,l = session.run([optimizer,loss],feed_dict=feed_dict)\n",
        "                    back_loss += l \n",
        "                    \n",
        "                    print(\"Mini-Batch - {} Back-Prop : {}, Loss : {}\".format(batch_num,back,l))\n",
        "                    batch_num +=1\n",
        "                back_loss /= batch_num\n",
        "                J.append(back_loss)\n",
        "                \n",
        "                #store the parameters in a dictionary\n",
        "                final_parameters['conv1_layer1_weights'] = session.run(conv1_layer1_weights)\n",
        "                final_parameters['conv1_layer2_weights'] = session.run(conv1_layer2_weights)\n",
        "                final_parameters['conv2_layer1_weights'] = session.run(conv2_layer1_weights)\n",
        "                final_parameters['conv2_layer2_weights'] = session.run(conv2_layer2_weights)\n",
        "                final_parameters['conv1_layer1_biases'] = session.run(conv1_layer1_biases)\n",
        "                final_parameters['conv1_layer2_biases'] = session.run(conv1_layer2_biases)\n",
        "                final_parameters['conv2_layer1_biases'] = session.run(conv2_layer1_biases)\n",
        "                final_parameters['conv2_layer2_biases'] = session.run(conv2_layer2_biases)\n",
        "                final_parameters['fc_layer1_weights'] = session.run(fc_layer1_weights)\n",
        "                final_parameters['fc_layer2_weights'] = session.run(fc_layer2_weights)\n",
        "                final_parameters['fc_layer1_biases'] = session.run(fc_layer1_biases)\n",
        "                final_parameters['fc_layer2_biases'] = session.run(fc_layer2_biases)\n",
        "                \n",
        "                #number of back-props\n",
        "                back+=1\n",
        "                \n",
        "                #make new memory \n",
        "                replay_memory = list()\n",
        "                replay_labels = list()\n",
        "                \n",
        "            \n",
        "            if(local_iters%400==0):\n",
        "                print(\"Episode : {}, Score : {}, Iters : {}, Finish : {}\".format(ep,total_score,local_iters,finish))\n",
        "            \n",
        "            local_iters += 1\n",
        "            total_iters += 1\n",
        "            \n",
        "        scores.append(total_score)\n",
        "        print(\"Episode {} finished with score {}, result : {} board : {}, epsilon  : {}, learning rate : {} \".format(ep,total_score,finish,board,epsilon,session.run(learning_rate)))\n",
        "        print()\n",
        "        \n",
        "        if((ep+1)%1000==0):\n",
        "            print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))    \n",
        "            print(\"Loss : {}\".format(J[len(J)-1]))\n",
        "            print()\n",
        "            \n",
        "        if(maximum<total_score):\n",
        "            maximum = total_score\n",
        "            episode = ep\n",
        "    print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Episode 0 finished with score 1320.0, result : lose board : [[  2.  16.   4.   2.]\n",
            " [  8.  32.  64.   4.]\n",
            " [  4. 128.   4.  16.]\n",
            " [  2.   4.   8.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 1 finished with score 572.0, result : lose board : [[ 2.  8.  2.  4.]\n",
            " [ 4. 64. 32.  2.]\n",
            " [ 8. 16.  2. 16.]\n",
            " [ 2.  4.  8.  4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 2 finished with score 980.0, result : lose board : [[  8.   4.   8.   2.]\n",
            " [  4.  32. 128.   4.]\n",
            " [  2.   8.   2.  16.]\n",
            " [  4.   2.   4.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 3 finished with score 688.0, result : lose board : [[ 4.  8. 16.  4.]\n",
            " [64.  4.  2. 32.]\n",
            " [ 4. 16.  8.  2.]\n",
            " [ 2.  8.  2. 32.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 4 finished with score 1348.0, result : lose board : [[  4.   2.  16.   8.]\n",
            " [  2.   8. 128.  32.]\n",
            " [ 64.  16.   4.   2.]\n",
            " [  8.   4.   2.   8.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 5 finished with score 1248.0, result : lose board : [[4.0, 2.0, 4.0, 2.0], [2.0, 32.0, 64.0, 8.0], [128.0, 2.0, 8.0, 4], [2, 16.0, 4.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 6 finished with score 664.0, result : lose board : [[ 2. 16.  4.  2.]\n",
            " [ 4.  8. 64. 32.]\n",
            " [ 2.  4. 32.  8.]\n",
            " [ 4.  8.  4.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 7 finished with score 1188.0, result : lose board : [[  2.   8.  16.   2.]\n",
            " [ 32.   4.  32.   4.]\n",
            " [  8. 128.  16.   8.]\n",
            " [  2.  16.   4.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 8 finished with score 736.0, result : lose board : [[2.0, 16.0, 8.0, 2.0], [16.0, 64.0, 32.0, 8.0], [8.0, 16.0, 8.0, 16.0], [2, 4.0, 16.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 9 finished with score 1592.0, result : lose board : [[  2.   4.  16.   2.]\n",
            " [  8.  64.   2. 128.]\n",
            " [ 16.  32.   4.   2.]\n",
            " [  2.  64.   2.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 10 finished with score 616.0, result : lose board : [[2, 4.0, 8.0, 4.0], [16.0, 2.0, 16.0, 32.0], [8.0, 16.0, 64.0, 4.0], [2.0, 8.0, 4.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 11 finished with score 1296.0, result : lose board : [[4.0, 128.0, 2.0, 4], [2.0, 64.0, 4.0, 2.0], [16.0, 4.0, 16.0, 4.0], [2.0, 32.0, 4.0, 8.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 12 finished with score 1292.0, result : lose board : [[  2.   8. 128.   2.]\n",
            " [  8.   2.   8.   4.]\n",
            " [  2.  32.  64.  16.]\n",
            " [  4.   8.   2.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 13 finished with score 712.0, result : lose board : [[ 2.  8. 16.  2.]\n",
            " [ 4. 32. 64.  8.]\n",
            " [ 8.  4. 32.  4.]\n",
            " [ 4. 16.  4.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 14 finished with score 1348.0, result : lose board : [[  4.   8.   2.   4.]\n",
            " [ 16.  64.   4.   2.]\n",
            " [  4.  16.  32. 128.]\n",
            " [  2.   8.   2.  16.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 15 finished with score 1012.0, result : lose board : [[2.0, 8.0, 16.0, 8.0], [4.0, 64.0, 32.0, 2], [2.0, 32.0, 2.0, 16.0], [4.0, 2.0, 64.0, 4.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 16 finished with score 1008.0, result : lose board : [[  2.   4.   8.   4.]\n",
            " [  4.  16. 128.   8.]\n",
            " [ 16.  32.   8.   2.]\n",
            " [  2.   4.   2.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 17 finished with score 1308.0, result : lose board : [[2.0, 4.0, 16.0, 4.0], [16.0, 128.0, 64.0, 2.0], [4.0, 8.0, 32.0, 4.0], [2, 4, 8, 2]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 18 finished with score 900.0, result : lose board : [[2.0, 4.0, 8.0, 2.0], [16.0, 64.0, 32.0, 64.0], [4.0, 2.0, 8.0, 16.0], [2.0, 8.0, 4.0, 2]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 19 finished with score 612.0, result : lose board : [[ 4. 32.  2.  4.]\n",
            " [ 8.  4.  8. 16.]\n",
            " [ 4. 16. 64.  2.]\n",
            " [ 2.  4. 16.  4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 20 finished with score 1052.0, result : lose board : [[  4. 128.  16.   2.]\n",
            " [ 16.  32.   4.   8.]\n",
            " [  8.   2.   8.   4.]\n",
            " [  2.   4.  16.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 21 finished with score 1304.0, result : lose board : [[16.0, 2.0, 4, 2], [2.0, 8.0, 128.0, 4.0], [8.0, 32.0, 64.0, 2.0], [2.0, 16.0, 4.0, 8.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 22 finished with score 1212.0, result : lose board : [[  2.   4.  16.   2.]\n",
            " [  8. 128.   2.   4.]\n",
            " [  2.  64.   4.   8.]\n",
            " [  8.   4.  16.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 23 finished with score 2072.0, result : lose board : [[2.0, 32.0, 4.0, 2.0], [256.0, 4.0, 8.0, 4.0], [2.0, 16.0, 32.0, 8.0], [4.0, 2.0, 8.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 24 finished with score 852.0, result : lose board : [[4.0, 2.0, 4.0, 2.0], [2.0, 32.0, 64.0, 8.0], [64.0, 16.0, 4.0, 2.0], [2, 4.0, 16.0, 4.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 25 finished with score 560.0, result : lose board : [[16.  4.  8.  2.]\n",
            " [ 4. 32. 64.  8.]\n",
            " [ 8.  2.  8.  4.]\n",
            " [ 2.  8.  4.  2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 26 finished with score 1344.0, result : lose board : [[4.0, 2.0, 16.0, 2.0], [2, 8.0, 128.0, 8.0], [4.0, 64.0, 8.0, 16.0], [2.0, 4.0, 32.0, 4.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 27 finished with score 968.0, result : lose board : [[2, 4.0, 64.0, 8.0], [4.0, 8.0, 32.0, 2.0], [8.0, 32.0, 4.0, 8.0], [2.0, 64.0, 8.0, 4.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 28 finished with score 864.0, result : lose board : [[ 2. 16. 32.  2.]\n",
            " [ 4. 64. 16.  8.]\n",
            " [64.  2.  4.  2.]\n",
            " [ 2.  8.  2.  4.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 29 finished with score 1596.0, result : lose board : [[2, 4.0, 8.0, 4.0], [4.0, 16.0, 64.0, 2.0], [8.0, 128.0, 8.0, 32.0], [2.0, 64.0, 2.0, 8.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 30 finished with score 1372.0, result : lose board : [[2, 8, 4.0, 2], [16.0, 128.0, 16.0, 8.0], [2.0, 8.0, 64.0, 16.0], [4.0, 2.0, 32.0, 8.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 31 finished with score 636.0, result : lose board : [[ 2.  8. 16.  2.]\n",
            " [ 4. 64. 32. 16.]\n",
            " [ 2.  8. 16.  4.]\n",
            " [ 8.  4.  8.  2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 32 finished with score 664.0, result : lose board : [[2, 8.0, 2.0, 4], [32.0, 16.0, 4.0, 2], [4.0, 32.0, 64.0, 16.0], [2.0, 8.0, 4.0, 2.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 33 finished with score 1316.0, result : lose board : [[  4.   2.  32.   2.]\n",
            " [  8.  16. 128.   8.]\n",
            " [  4.  64.  16.   4.]\n",
            " [  2.   4.   8.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 34 finished with score 644.0, result : lose board : [[ 2. 16.  2.  4.]\n",
            " [16. 32. 16. 64.]\n",
            " [ 4.  8.  2.  4.]\n",
            " [ 2. 16.  4.  2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 35 finished with score 1048.0, result : lose board : [[ 16.   4.   2.   4.]\n",
            " [  2. 128.  32.  16.]\n",
            " [  4.   8.  16.   8.]\n",
            " [  2.   4.   8.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 36 finished with score 664.0, result : lose board : [[ 8.  4.  2.  4.]\n",
            " [16.  8. 16.  2.]\n",
            " [ 8. 64. 32. 16.]\n",
            " [ 2. 16.  4.  2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 37 finished with score 2964.0, result : lose board : [[8.0, 16.0, 8.0, 4.0], [4.0, 64.0, 128.0, 16.0], [8.0, 256.0, 4.0, 2], [2, 4.0, 8.0, 4.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 38 finished with score 4596.0, result : lose board : [[  4.   2.   8.   4.]\n",
            " [  2.  16.  32.  16.]\n",
            " [  4.  64. 512.   4.]\n",
            " [  2.   4.   8.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 39 finished with score 780.0, result : lose board : [[2.0, 4.0, 2.0, 16.0], [8.0, 32.0, 4.0, 32.0], [4.0, 64.0, 32.0, 4.0], [2.0, 8.0, 4.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 40 finished with score 1144.0, result : lose board : [[8.0, 32.0, 4.0, 2.0], [16.0, 8.0, 128.0, 8.0], [4.0, 16.0, 8.0, 2], [2, 4.0, 32.0, 4.0]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoY-PjeJtzFm"
      },
      "source": [
        "### Store the Trained Weights in a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEcg8pSutzFn"
      },
      "source": [
        "path = r'E:\\Study n Work\\Projects\\2048\\Final Weights'\n",
        "weights = ['conv1_layer1_weights','conv1_layer2_weights','conv2_layer1_weights','conv2_layer2_weights','fc_layer1_weights','fc_layer1_biases','fc_layer2_weights','fc_layer2_biases']\n",
        "for w in weights:\n",
        "    flatten = final_parameters[w].reshape(-1,1)\n",
        "    file = open(path + '\\\\' + w +'.csv','w')\n",
        "    file.write('Sno,Weight\\n')\n",
        "    for i in range(flatten.shape[0]):\n",
        "        file.write(str(i) +',' +str(flatten[i][0])+'\\n') \n",
        "    file.close()\n",
        "    print(w + \" written!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgKDmZ0MtzFt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
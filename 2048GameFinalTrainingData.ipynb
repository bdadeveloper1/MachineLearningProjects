{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2048GameFinalTrainingData",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmG4L20roCG4kjX2sW3gui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdadeveloper1/MachineLearningProjects/blob/main/2048GameFinalTrainingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AicJjsg2yFj1"
      },
      "source": [
        "#World Data Science Institute Internship\n",
        "#2048 Game Implementation using Deep Reinforcement Learning (Training Data Creation)\n",
        "\n",
        "By Brandon Oppong-Antwi\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Important Notes on Installations:\n",
        "\n",
        "Python Version: Python 3.8.3\n",
        "\n",
        "Tensorflow Version: Tensorflow 2.3.0\n",
        "\n",
        "Operating System: Windows 10 \n",
        "\n",
        "It is important to install the correct version of Tensorflow and Python for optimal GPU support in order to run this program. Additional Installation and GPU support can be found at: https://www.tensorflow.org/install/gpu\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Note: For testing if you would like the data to formulate quick data that is good enough for testing you can use an M = 1000. I used an M = 200001 which in turn completed the game to 2048. This took more than two days to complete on my Lenovo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNKgwCKp3XNx"
      },
      "source": [
        "### Required Imported Libraries\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAdUtN9px-Cs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "33dccac7-e052-4271-fd2b-588f8f46802a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "import numpy as np\n",
        "from copy import deepcopy #keep memory for dictionary of objects\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfdddc8M4zdA"
      },
      "source": [
        "### Game Theory and Logic\n",
        "\n",
        "For those who have not played, 2048 is a game that is played on a 4 x 4 grid where the goal is for the the user to merge and collide the tiles on the board until they reach the desired final result 2048.\n",
        "\n",
        "The user  has the option to  move in the four cardinal directions and after every move a new tile is generated randomly in the grid which is either numbered 2 or 4 with a probability of about 0.90 or 0.10 respectively. A move is legal if at least one tile can be slid into an empty spot or if the tiles\n",
        "can be combined in the chosen direction. The game ends when the user does not have any legal moves left. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvtSMiGRaljl"
      },
      "source": [
        "#Initialize and start a new game, this will create the empty grid environment with a 2 or 4 initially in the grid.\n",
        "\n",
        "def newGame(n): #new game\n",
        "    matrix = np.zeros([n,n]) #grid will be represented in the form a matrix\n",
        "    return matrix\n",
        "\n",
        "#add two or 4 in the matrix. This is the initial start of the game where only two or 4 appear. Each new game instance will start with a 2 or 4\n",
        "def addTwo(mat): \n",
        "    emptyTiles = []\n",
        "    for i in range(len(mat)):\n",
        "        for j in range(len(mat[0])):\n",
        "            if(mat[i][j]==0):\n",
        "                emptyTiles.append((i,j)) #append the array of empty cells to include the values\n",
        "    if(len(emptyTiles)==0):\n",
        "        return mat\n",
        "    \n",
        "    indexPair = emptyTiles[random.randint(0,len(emptyTiles)-1)]\n",
        "    \n",
        "    probability = random.random() #allow the probabilities to be randomly generated with 90% change of getting a 2 and 10% chance of getting 4\n",
        "    if(probability>=0.9):\n",
        "        mat[indexPair[0]][indexPair[1]]=4\n",
        "    else:\n",
        "        mat[indexPair[0]][indexPair[1]]=2\n",
        "    return mat\n",
        "\n",
        "#check the state of the game and where the game is currently at\n",
        "def gameState(mat):\n",
        "    #if 2048 in mat:\n",
        "    #    return 'win'\n",
        "    \n",
        "    for i in range(len(mat)-1): #intentionally reduced to check the row on the right and below\n",
        "        for j in range(len(mat[0])-1): #more elegant to use exceptions but most likely this will be the solution\n",
        "            if mat[i][j]==mat[i+1][j] or mat[i][j+1]==mat[i][j]:\n",
        "                return 'not done'\n",
        "            \n",
        "    for i in range(len(mat)): #check for any zero entries\n",
        "        for j in range(len(mat[0])):\n",
        "            if mat[i][j]==0:\n",
        "                return 'not done'\n",
        "            \n",
        "    for k in range(len(mat)-1): #to check the left/right entries on the last row\n",
        "        if mat[len(mat)-1][k]==mat[len(mat)-1][k+1]:\n",
        "            return 'not done'\n",
        "        \n",
        "    for j in range(len(mat)-1): #check up/down entries on last column\n",
        "        if mat[j][len(mat)-1]==mat[j+1][len(mat)-1]:\n",
        "            return 'not done'\n",
        "        \n",
        "    return 'game over' # returns lose if the desire result is not acquired\n",
        "\n",
        "\n",
        "#Game Functionality\n",
        "\n",
        "def transpose(mat): #Simulation for tiles changing place with one another(Transposing)\n",
        "    new=[]\n",
        "    for i in range(len(mat[0])):\n",
        "        new.append([])\n",
        "        for j in range(len(mat)):\n",
        "            new[i].append(mat[j][i])\n",
        "            \n",
        "    return np.transpose(mat)\n",
        "\n",
        "def reverse(mat): #Simulation for if the user went in the reverse direction\n",
        "    new=[]\n",
        "    for i in range(len(mat)):\n",
        "        new.append([])\n",
        "        for j in range(len(mat[0])):\n",
        "            new[i].append(mat[i][len(mat[0])-j-1])\n",
        "    return new\n",
        "\n",
        "def coverUp(mat): #Simulation for covering Up the tiles when changed in the grid\n",
        "    new = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "    done = False\n",
        "    for i in range(4):\n",
        "        count = 0\n",
        "        for j in range(4):\n",
        "            if mat[i][j]!=0:\n",
        "                new[i][count] = mat[i][j]\n",
        "                if j!=count:\n",
        "                    done=True\n",
        "                count+=1\n",
        "    return (new,done)\n",
        "\n",
        "\n",
        "def merge(mat): #Simulation for merging the Tiles together... Important so that tile can be changed\n",
        "    done=False\n",
        "    score = 0\n",
        "    for i in range(4):\n",
        "        for j in range(3):\n",
        "            if mat[i][j]==mat[i][j+1] and mat[i][j]!=0:\n",
        "                mat[i][j]*=2\n",
        "                score += mat[i][j]   \n",
        "                mat[i][j+1]=0\n",
        "                done=True\n",
        "    return (mat,done,score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fxMPrggw8lP"
      },
      "source": [
        "##### Game Controls\n",
        "*   User instance directions that will reflect game functionality methods from earlier\n",
        "*   Finds the Empty Cell Function that will be used in the reward\n",
        "\n",
        "*   Convert the input values\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asExYIUqnhIC"
      },
      "source": [
        "\n",
        "\n",
        "#Movement for the up direction\n",
        "def up(game):\n",
        "        game = transpose(game)\n",
        "        game,done = coverUp(game)\n",
        "        temp = merge(game)\n",
        "        game = temp[0]\n",
        "        done = done or temp[1]\n",
        "        game = coverUp(game)[0]\n",
        "        game = transpose(game)\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#Movement for the down direction\n",
        "def down(game):\n",
        "        game=reverse(transpose(game))\n",
        "        game,done=coverUp(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=coverUp(game)[0]\n",
        "        game=transpose(reverse(game))\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#Movement for the left direction\n",
        "def left(game):\n",
        "        game,done=coverUp(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=coverUp(game)[0]\n",
        "        return (game,done,temp[2])\n",
        "\n",
        "#Movement for the right direction\n",
        "def right(game):\n",
        "        game=reverse(game)\n",
        "        game,done=coverUp(game)\n",
        "        temp=merge(game)\n",
        "        game=temp[0]\n",
        "        done=done or temp[1]\n",
        "        game=coverUp(game)[0]\n",
        "        game=reverse(game)\n",
        "        return (game,done,temp[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N9nlqXqnhJz"
      },
      "source": [
        "controls = {0:up,1:left,2:right,3:down}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkdZyVH1alju"
      },
      "source": [
        "#convert the input game matrix into corresponding power of 2 matrix.\n",
        "def changeValues(X):\n",
        "    powerMatrix = np.zeros(shape=(1,4,4,16),dtype=np.float32)\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            if(X[i][j]==0):\n",
        "                powerMatrix[0][i][j][0] = 1.0\n",
        "            else:\n",
        "                power = int(math.log(X[i][j],2))\n",
        "                powerMatrix[0][i][j][power] = 1.0\n",
        "    return powerMatrix        \n",
        "\n",
        "#find and keep track of the the number of empty cells in the game matrix that are still remaining.\n",
        "def findemptyCell(mat):\n",
        "    count = 0\n",
        "    for i in range(len(mat)):\n",
        "        for j in range(len(mat)):\n",
        "            if(mat[i][j]==0):\n",
        "                count+=1\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ua82wUydmf"
      },
      "source": [
        "Hyper Parameters used for training data\n",
        "\n",
        "Hyper-Parameterization: Tuned hyperparameters play a large role in eliciting the best results for the algorithm. Hyperparameters are used before training and will directly control the behavior of the training. Choosing the most efficient Hyper parameters, which I have put below play an integral role in the success of the neural network architecture.The two most important are the learning rate and the network size which will effect the speed of the network and how many layers are in our network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_W6Z-4Oalj2"
      },
      "source": [
        "#Learning rate- Can edit depending, but this is a good learning rate in that it will not overshoot the data and give a reasonable amount of epochs.\n",
        "learningRate = 0.0005\n",
        "\n",
        "#gamma for Q-learning-Gamma (γ) is a number between [0,1] and its used to discount the reward as the time passes\n",
        "gamma = 0.9\n",
        "\n",
        "#epsilon greedy approach\n",
        "epsilon = 0.9\n",
        "\n",
        "#to store states and lables of the game for training\n",
        "#states of the game\n",
        "replayMemory = list()\n",
        "\n",
        "#labels of the states\n",
        "replayLabels = list()\n",
        "\n",
        "#capacity of memory\n",
        "mem_capacity = 6000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFyVnCj3Bw_O"
      },
      "source": [
        "Network Architecture\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNMax5FKZ3oE"
      },
      "source": [
        "\n",
        "Convolution Neural Network Architecture\n",
        "\n",
        "In this section, we will use front propogation with our policy network.The  Policy Network controlling the actions in 2048. We explain the\n",
        "game playing with front-propagation algorithm and we use the  exploration process to  an ϵ-greedy algorithm. \n",
        "\n",
        "\n",
        "![network architecture](https://github.com/navjindervirdee/2048-deep-reinforcement-learning/raw/master/Architecture/Architecture.JPG?raw=true)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tOJGCuEalj8"
      },
      "source": [
        "#first convolution layer depth- setting the number of hidden units larger than the number of inputs tends to enable better results in number of tasks\n",
        "firstDepth = 128\n",
        "\n",
        "#second convolution layer depth- Convolutional Neural Networks (CNN) tend to perform better with the amount of layers added.In order for this model to run well a 3 layer network consiting of\n",
        "#two hidden layers makes this optimal\n",
        "secondDepth = 128\n",
        "\n",
        "#batch size for batch gradient descent- as an effect on the resource requirements of the training process, speed and number of iterations in a non-trivial way.\n",
        "batchSize = 512\n",
        "\n",
        "#input units\n",
        "input_units = 16\n",
        "\n",
        "#fully connected layer neurons\n",
        "hidden_units = 256\n",
        "\n",
        "#output neurons = number of moves\n",
        "output_units = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLAiRogbe60n"
      },
      "source": [
        "Activation Function and Optimizers\n",
        "\n",
        "\n",
        "*   Activation - RELU\n",
        "  \n",
        "    Following each of the two hidden layers, we use the ReLu h : x → (x) activation function which will help to ensure that we can get non vanishing gradients.\n",
        "\n",
        "![picture](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS8qyQ46SFPNxoXPQWJKJ8fMA7rnSA3jWOQog&usqp=CAU)\n",
        "\n",
        "\n",
        "*  Optimizer - RMSPRop\n",
        "\n",
        "    This optimizer is a gradient based technique which  balances the step size  (momentum),  decreasing the step for large gradients to avoid exploding, and increasing the step for small gradients to avoid vanishing. \n",
        "\n",
        "![picture](https://miro.medium.com/max/1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx6G2c_OybJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "43c7a98c-ed66-4522-c571-afff49e66c44"
      },
      "source": [
        "#input data\n",
        "tfBatchDataset = tf.placeholder(tf.float32,shape=(batchSize,4,4,16))\n",
        "tfBatchLabels  = tf.placeholder(tf.float32,shape=(batchSize,output_units))\n",
        "\n",
        "datasetSingle   = tf.placeholder(tf.float32,shape=(1,4,4,16))\n",
        "\n",
        "\n",
        "#CONV LAYERS\n",
        "#conv layer1 weights\n",
        "conv1_layer1_weights = tf.Variable(tf.truncated_normal([1,2,input_units,firstDepth],mean=0,stddev=0.01))\n",
        "conv2_layer1_weights = tf.Variable(tf.truncated_normal([2,1,input_units,firstDepth],mean=0,stddev=0.01))\n",
        "\n",
        "#conv layer2 weights\n",
        "conv1_layer2_weights = tf.Variable(tf.truncated_normal([1,2,firstDepth,secondDepth],mean=0,stddev=0.01))\n",
        "conv2_layer2_weights = tf.Variable(tf.truncated_normal([2,1,firstDepth,secondDepth],mean=0,stddev=0.01))\n",
        "\n",
        "\n",
        "\n",
        "#FUllY CONNECTED LAYERS\n",
        "expand_size = 2*4*secondDepth*2 + 3*3*secondDepth*2 + 4*3*firstDepth*2\n",
        "fc_layer1_weights = tf.Variable(tf.truncated_normal([expand_size,hidden_units],mean=0,stddev=0.01))\n",
        "fc_layer1_biases = tf.Variable(tf.truncated_normal([1,hidden_units],mean=0,stddev=0.01))\n",
        "fc_layer2_weights = tf.Variable(tf.truncated_normal([hidden_units,output_units],mean=0,stddev=0.01))\n",
        "fc_layer2_biases = tf.Variable(tf.truncated_normal([1,output_units],mean=0,stddev=0.01))\n",
        "\n",
        "\n",
        "#model\n",
        "def model(dataset):\n",
        "    #layer1\n",
        "    conv1 = tf.nn.conv2d(dataset,conv1_layer1_weights,[1,1,1,1],padding='VALID') \n",
        "    conv2 = tf.nn.conv2d(dataset,conv2_layer1_weights,[1,1,1,1],padding='VALID') \n",
        "    \n",
        "    #layer1 relu activation\n",
        "    relu1 = tf.nn.relu(conv1)\n",
        "    relu2 = tf.nn.relu(conv2)\n",
        "    \n",
        "    #layer2\n",
        "    conv11 = tf.nn.conv2d(relu1,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "    conv12 = tf.nn.conv2d(relu1,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "\n",
        "    conv21 = tf.nn.conv2d(relu2,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "    conv22 = tf.nn.conv2d(relu2,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
        "\n",
        "    #layer2 relu activation\n",
        "    relu11 = tf.nn.relu(conv11)\n",
        "    relu12 = tf.nn.relu(conv12)\n",
        "    relu21 = tf.nn.relu(conv21)\n",
        "    relu22 = tf.nn.relu(conv22)\n",
        "    \n",
        "    #get shapes of all activations\n",
        "    shape1 = relu1.get_shape().as_list()\n",
        "    shape2 = relu2.get_shape().as_list()\n",
        "    \n",
        "    shape11 = relu11.get_shape().as_list()\n",
        "    shape12 = relu12.get_shape().as_list()\n",
        "    shape21 = relu21.get_shape().as_list()\n",
        "    shape22 = relu22.get_shape().as_list()\n",
        "\n",
        "    #expansion\n",
        "    hidden1 = tf.reshape(relu1,[shape1[0],shape1[1]*shape1[2]*shape1[3]])\n",
        "    hidden2 = tf.reshape(relu2,[shape2[0],shape2[1]*shape2[2]*shape2[3]])\n",
        "    \n",
        "    hidden11 = tf.reshape(relu11,[shape11[0],shape11[1]*shape11[2]*shape11[3]])\n",
        "    hidden12 = tf.reshape(relu12,[shape12[0],shape12[1]*shape12[2]*shape12[3]])\n",
        "    hidden21 = tf.reshape(relu21,[shape21[0],shape21[1]*shape21[2]*shape21[3]])\n",
        "    hidden22 = tf.reshape(relu22,[shape22[0],shape22[1]*shape22[2]*shape22[3]])\n",
        "\n",
        "    #concatenation\n",
        "    hidden = tf.concat([hidden1,hidden2,hidden11,hidden12,hidden21,hidden22],axis=1)\n",
        "\n",
        "    #full connected layers\n",
        "    hidden = tf.matmul(hidden,fc_layer1_weights) + fc_layer1_biases\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "\n",
        "    #output layer\n",
        "    output = tf.matmul(hidden,fc_layer2_weights) + fc_layer2_biases\n",
        "    \n",
        "    #return output\n",
        "    return output\n",
        "\n",
        "#for single example\n",
        "single_output = model(datasetSingle)\n",
        "\n",
        "#for batch data\n",
        "logits = model(tfBatchDataset)\n",
        "\n",
        "#loss\n",
        "loss = tf.square(tf.subtract(tfBatchLabels,logits))\n",
        "loss = tf.reduce_sum(loss,axis=1,keep_dims=True)\n",
        "loss = tf.reduce_mean(loss)/2.0\n",
        "\n",
        "#optimizer\n",
        "global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "learning_rate = tf.train.exponential_decay(float(learningRate), global_step, 1000, 0.90, staircase=True)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ9bw0tcnvlt"
      },
      "source": [
        "#loss\n",
        "J = []\n",
        "\n",
        "#scores\n",
        "scores = []\n",
        "\n",
        "#to store final parameters\n",
        "final_parameters = {}\n",
        "\n",
        "#number of episodes- I am stating that an episode is one a sequence of states, actions and rewards, which ends with terminal state, so think one pass for the game.\n",
        "#This M number is important in order to gain all possible solutions for training\n",
        "M = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxzR5o7UzfK"
      },
      "source": [
        "### Create Training Data Set\n",
        "\n",
        "The reward algorithm fucntion will be as follows \n",
        "\n",
        "Q(reward) = number of merges + log(new max,2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B23lGaeralkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3874d26d-8add-4a4e-9f52-7993fa6973e3"
      },
      "source": [
        "with tf.Session() as session:\n",
        "    tf.global_variables_initializer().run()\n",
        "    print(\"Initialized\")\n",
        "    \n",
        "    global epsilon\n",
        "    global replayLabels\n",
        "    global replayMemory\n",
        "\n",
        "    #for episode with max score\n",
        "    maximum = -1\n",
        "    episode = -1\n",
        "    \n",
        "    #total_iters \n",
        "    total_iters = 1\n",
        "    \n",
        "    #number of back props\n",
        "    back=0\n",
        "    \n",
        "    for ep in range(M):\n",
        "        global board\n",
        "        board = newGame(4)\n",
        "        addTwo(board)\n",
        "        addTwo(board)\n",
        "        \n",
        "        #whether episode finished or not\n",
        "        finish = 'not done'\n",
        "        \n",
        "        #total_score of this episode\n",
        "        total_score = 0\n",
        "        \n",
        "        #iters per episode\n",
        "        local_iters = 1\n",
        "        \n",
        "        while(finish=='not done'):\n",
        "            prev_board = deepcopy(board)\n",
        "            \n",
        "            #gets the required move for this state\n",
        "            state = deepcopy(board)\n",
        "            state = changeValues(state)\n",
        "            state = np.array(state,dtype = np.float32).reshape(1,4,4,16)\n",
        "            feed_dict = {datasetSingle:state}\n",
        "            control_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "            \n",
        "            #find the move with max Q value\n",
        "            control_buttons = np.flip(np.argsort(control_scores),axis=1)\n",
        "            \n",
        "            #copy the Q-values as labels\n",
        "            labels = deepcopy(control_scores[0])\n",
        "            \n",
        "            #generate random number for epsilon greedy approach\n",
        "            num = random.uniform(0,1)\n",
        "            \n",
        "            #store prev max\n",
        "            prev_max = np.max(prev_board)\n",
        "            \n",
        "            #num is less epsilon generate random move\n",
        "            if(num<epsilon):\n",
        "                #find legal moves\n",
        "                legal_moves = list()\n",
        "                for i in range(4):\n",
        "                    temp_board = deepcopy(prev_board)\n",
        "                    temp_board,_,_ = controls[i](temp_board)\n",
        "                    if(np.array_equal(temp_board,prev_board)):\n",
        "                        continue\n",
        "                    else:\n",
        "                        legal_moves.append(i)\n",
        "                if(len(legal_moves)==0):\n",
        "                    finish = 'lose'\n",
        "                    continue\n",
        "                \n",
        "                #generate random move.\n",
        "                con = random.sample(legal_moves,1)[0]\n",
        "                \n",
        "                #apply the move\n",
        "                temp_state = deepcopy(prev_board)\n",
        "                temp_state,_,score = controls[con](temp_state)\n",
        "                total_score += score\n",
        "                finish = gameState(temp_state)\n",
        "                \n",
        "                #get number of merges\n",
        "                empty1 = findemptyCell(prev_board)\n",
        "                empty2 = findemptyCell(temp_state)\n",
        "                \n",
        "                if(finish=='not done'):\n",
        "                    temp_state = addTwo(temp_state)\n",
        "\n",
        "                board = deepcopy(temp_state)\n",
        "\n",
        "                #get next max after applying the move\n",
        "                next_max = np.max(temp_state)\n",
        "                \n",
        "                #reward math.log(next_max,2)*0.1 if next_max is higher than prev max\n",
        "                labels[con] = (math.log(next_max,2))*0.1\n",
        "                \n",
        "                if(next_max==prev_max):\n",
        "                    labels[con] = 0\n",
        "                \n",
        "                #reward is also the number of merges\n",
        "                labels[con] += (empty2-empty1)\n",
        "                \n",
        "                #get the next state max Q-value\n",
        "                temp_state = changeValues(temp_state)\n",
        "                temp_state = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
        "                feed_dict = {datasetSingle:temp_state}\n",
        "                temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "                    \n",
        "                max_qvalue = np.max(temp_scores)\n",
        "                \n",
        "                #final labels add gamma*max_qvalue\n",
        "                labels[con] = (labels[con] + gamma*max_qvalue)\n",
        "            \n",
        "            #Generates the maximum predicted move\n",
        "            else:\n",
        "                for con in control_buttons[0]:\n",
        "                    prev_state = deepcopy(prev_board)\n",
        "                    \n",
        "                    #apply the LEGAl Move with max q_value\n",
        "                    temp_state,_,score = controls[con](prev_state)\n",
        "                    \n",
        "                    #if illegal move label = 0\n",
        "                    if(np.array_equal(prev_board,temp_state)):\n",
        "                        labels[con] = 0\n",
        "                        continue\n",
        "                        \n",
        "                    #Calculates the number of merges the computer took\n",
        "                    empty1 = findemptyCell(prev_board)\n",
        "                    empty2 = findemptyCell(temp_state)\n",
        "\n",
        "                    \n",
        "                    temp_state = addTwo(temp_state)\n",
        "                    board = deepcopy(temp_state)\n",
        "                    total_score += score\n",
        "\n",
        "                    next_max = np.max(temp_state)\n",
        "                    \n",
        "                    #Rewrd process\n",
        "                    labels[con] = (math.log(next_max,2))*0.1\n",
        "                    if(next_max==prev_max):\n",
        "                        labels[con] = 0\n",
        "                    \n",
        "                    labels[con] += (empty2-empty1)\n",
        "\n",
        "                    #get next max qvalue\n",
        "                    temp_state = changeValues(temp_state)\n",
        "                    temp_state = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
        "                    feed_dict = {datasetSingle:temp_state}\n",
        "                    temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
        "\n",
        "                    max_qvalue = np.max(temp_scores)\n",
        "\n",
        "                    #final labels\n",
        "                    labels[con] = (labels[con] + gamma*max_qvalue)\n",
        "                    break\n",
        "                    \n",
        "                if(np.array_equal(prev_board,board)):\n",
        "                    finish = 'lose'\n",
        "            \n",
        "            #decrease the epsilon value\n",
        "            if((ep>10000) or (epsilon>0.1 and total_iters%2500==0)):\n",
        "                epsilon = epsilon/1.005\n",
        "                \n",
        "           \n",
        "            #change the matrix values and store them in memory\n",
        "            prev_state = deepcopy(prev_board)\n",
        "            prev_state = changeValues(prev_state)\n",
        "            prev_state = np.array(prev_state,dtype=np.float32).reshape(1,4,4,16)\n",
        "            replayLabels.append(labels)\n",
        "            replayMemory.append(prev_state)\n",
        "            \n",
        "            \n",
        "            #back-propagation\n",
        "            if(len(replayMemory)>=mem_capacity):\n",
        "                back_loss = 0\n",
        "                batch_num = 0\n",
        "                z = list(zip(replayMemory,replayLabels))\n",
        "                np.random.shuffle(z)\n",
        "                np.random.shuffle(z)\n",
        "                replayMemory,replayLabels = zip(*z)\n",
        "                \n",
        "                for i in range(0,len(replayMemory),batchSize):\n",
        "                    if(i + batchSize>len(replayMemory)):\n",
        "                        break\n",
        "                        \n",
        "                    batch_data = deepcopy(replayMemory[i:i+batchSize])\n",
        "                    batch_labels = deepcopy(replayLabels[i:i+batchSize])\n",
        "                    \n",
        "                    batch_data = np.array(batch_data,dtype=np.float32).reshape(batchSize,4,4,16)\n",
        "                    batch_labels = np.array(batch_labels,dtype=np.float32).reshape(batchSize,output_units)\n",
        "                \n",
        "                    feed_dict = {tfBatchDataset: batch_data, tfBatchLabels: batch_labels}\n",
        "                    _,l = session.run([optimizer,loss],feed_dict=feed_dict)\n",
        "                    back_loss += l \n",
        "                    \n",
        "                    print(\"Mini-Batch - {} Back-Prop : {}, Loss : {}\".format(batch_num,back,l))\n",
        "                    batch_num +=1\n",
        "                back_loss /= batch_num\n",
        "                J.append(back_loss)\n",
        "                \n",
        "                #store the parameters in a dictionary\n",
        "                #In the network architecture I deleted the biases because they negatively affected my final ouput\n",
        "                final_parameters['conv1_layer1_weights'] = session.run(conv1_layer1_weights)\n",
        "                final_parameters['conv1_layer2_weights'] = session.run(conv1_layer2_weights)\n",
        "                final_parameters['conv2_layer1_weights'] = session.run(conv2_layer1_weights)\n",
        "                final_parameters['conv2_layer2_weights'] = session.run(conv2_layer2_weights)\n",
        "                final_parameters['fc_layer1_weights'] = session.run(fc_layer1_weights)\n",
        "                final_parameters['fc_layer2_weights'] = session.run(fc_layer2_weights)\n",
        "                final_parameters['fc_layer1_biases'] = session.run(fc_layer1_biases)\n",
        "                final_parameters['fc_layer2_biases'] = session.run(fc_layer2_biases)\n",
        "                \n",
        "                #number of back-props\n",
        "                back+=1\n",
        "                \n",
        "                #make new memory \n",
        "                replayMemory = list()\n",
        "                replayLabels = list()\n",
        "                \n",
        "            \n",
        "            if(local_iters%400==0):\n",
        "                print(\"Episode : {}, Score : {}, Iters : {}, Finish : {}\".format(ep,total_score,local_iters,finish))\n",
        "            \n",
        "            local_iters += 1\n",
        "            total_iters += 1\n",
        "            \n",
        "        scores.append(total_score)\n",
        "        print(\"Episode {} finished with score {}, result : {} board : {}, epsilon  : {}, learning rate : {} \".format(ep,total_score,finish,board,epsilon,session.run(learning_rate)))\n",
        "        print()\n",
        "        \n",
        "        if((ep+1)%1000==0):\n",
        "            print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))    \n",
        "            print(\"Loss : {}\".format(J[len(J)-1]))\n",
        "            print()\n",
        "            \n",
        "        if(maximum<total_score):\n",
        "            maximum = total_score\n",
        "            episode = ep\n",
        "    print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Episode 0 finished with score 1364.0, result : lose board : [[  2.   4.   2.   4.]\n",
            " [  4.  16. 128.  16.]\n",
            " [  8.  64.  16.   4.]\n",
            " [  4.  32.   8.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 1 finished with score 2480.0, result : lose board : [[2, 4.0, 8, 2], [4.0, 2.0, 64.0, 16.0], [8.0, 64.0, 16.0, 2.0], [4.0, 256.0, 2.0, 4.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 2 finished with score 1152.0, result : lose board : [[  2.   8.  32.   4.]\n",
            " [  4.  16. 128.  16.]\n",
            " [  8.  32.  16.   8.]\n",
            " [  4.   2.   8.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 3 finished with score 1656.0, result : lose board : [[  2.   4.  64.   2.]\n",
            " [ 16.   8.  32.   8.]\n",
            " [  4. 128.   8.  64.]\n",
            " [  2.   8.  16.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 4 finished with score 660.0, result : lose board : [[ 2.  8.  2.  8.]\n",
            " [ 4.  2.  8. 64.]\n",
            " [ 2.  4. 32.  8.]\n",
            " [ 4.  2. 16. 32.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 5 finished with score 816.0, result : lose board : [[16.  2.  8.  2.]\n",
            " [ 2.  4. 32. 64.]\n",
            " [ 4. 32.  8. 16.]\n",
            " [ 2.  8. 32.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 6 finished with score 2208.0, result : lose board : [[  8.   2.   4.   2.]\n",
            " [  2.   4.  64.   8.]\n",
            " [ 16.   8. 256.   4.]\n",
            " [  2.   4.  16.   2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 7 finished with score 1324.0, result : lose board : [[2.0, 8.0, 32.0, 2.0], [8.0, 128.0, 2.0, 16.0], [2.0, 4.0, 64.0, 8.0], [4.0, 16.0, 4, 2]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 8 finished with score 1428.0, result : lose board : [[4.0, 16.0, 4.0, 2.0], [16.0, 2.0, 64.0, 32.0], [8.0, 128.0, 32.0, 8.0], [2.0, 8.0, 4.0, 2]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 9 finished with score 1228.0, result : lose board : [[4.0, 2.0, 32.0, 4], [2.0, 4.0, 128.0, 2.0], [8.0, 64.0, 4.0, 8.0], [2, 4.0, 8.0, 4.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 10 finished with score 768.0, result : lose board : [[4.0, 8.0, 4.0, 2], [64.0, 4.0, 16.0, 8.0], [4.0, 16.0, 64.0, 2.0], [2.0, 8.0, 2.0, 4.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 11 finished with score 1436.0, result : lose board : [[2.0, 16.0, 4.0, 2], [8.0, 32.0, 64.0, 8.0], [16.0, 4.0, 128.0, 4.0], [2, 8.0, 32.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 12 finished with score 592.0, result : lose board : [[ 2.  8.  4.  2.]\n",
            " [ 8. 16.  2.  4.]\n",
            " [ 4. 64.  8. 32.]\n",
            " [ 2.  8. 16.  4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 13 finished with score 1256.0, result : lose board : [[2, 4, 8.0, 4], [4.0, 16.0, 64.0, 128.0], [16.0, 8.0, 2.0, 8.0], [4.0, 2.0, 16.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 14 finished with score 1448.0, result : lose board : [[  8.  16.   8.  64.]\n",
            " [  2.   4.  32.   4.]\n",
            " [  4. 128.  16.   8.]\n",
            " [  2.   4.  32.   4.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 15 finished with score 1384.0, result : lose board : [[2.0, 4.0, 2.0, 4.0], [4.0, 64.0, 8.0, 16.0], [32.0, 4.0, 128.0, 8.0], [4, 32.0, 4, 2]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 16 finished with score 564.0, result : lose board : [[2, 4.0, 8.0, 4.0], [4.0, 8.0, 16.0, 8.0], [2.0, 64.0, 32.0, 4.0], [8.0, 2.0, 8.0, 2.0]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 17 finished with score 520.0, result : lose board : [[ 4.  2. 32.  4.]\n",
            " [ 2. 64.  4.  2.]\n",
            " [ 4.  2. 16.  8.]\n",
            " [ 2.  8.  4.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 18 finished with score 592.0, result : lose board : [[ 2.  4. 32.  4.]\n",
            " [ 4.  8. 64.  8.]\n",
            " [ 8. 16.  2.  4.]\n",
            " [ 2.  4. 16.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 19 finished with score 448.0, result : lose board : [[ 2.  4.  8.  4.]\n",
            " [ 4. 16. 32. 16.]\n",
            " [16. 32.  8.  4.]\n",
            " [ 2.  8.  4.  2.]], epsilon  : 0.9, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 20 finished with score 600.0, result : lose board : [[2, 8.0, 2.0, 64.0], [4.0, 16.0, 8.0, 4.0], [8.0, 4.0, 32.0, 8.0], [2.0, 16.0, 4.0, 2.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 21 finished with score 1024.0, result : lose board : [[8.0, 2.0, 16.0, 8.0], [2.0, 32.0, 128.0, 2.0], [4.0, 8.0, 16.0, 4.0], [2, 4.0, 8.0, 2.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 22 finished with score 1084.0, result : lose board : [[4.0, 2.0, 32.0, 4.0], [8.0, 4.0, 16.0, 8.0], [4.0, 128.0, 8.0, 16.0], [2, 8.0, 16.0, 2.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 23 finished with score 1560.0, result : lose board : [[  4.   2.  16.   2.]\n",
            " [  8.  64. 128.   8.]\n",
            " [ 32.   2.   8.   4.]\n",
            " [  2.   4.  64.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 24 finished with score 736.0, result : lose board : [[ 2.  4. 32.  2.]\n",
            " [ 8. 16. 64.  8.]\n",
            " [16. 32.  8.  4.]\n",
            " [ 2.  4.  2. 16.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 25 finished with score 1372.0, result : lose board : [[4.0, 2.0, 4.0, 2.0], [2.0, 32.0, 64.0, 16.0], [32.0, 8.0, 2.0, 8.0], [4.0, 128.0, 4.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 26 finished with score 772.0, result : lose board : [[4.0, 16.0, 8.0, 2.0], [16.0, 64.0, 32.0, 4.0], [4.0, 32.0, 8.0, 16.0], [2.0, 16.0, 4.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 27 finished with score 1504.0, result : lose board : [[4.0, 8.0, 4.0, 2.0], [32.0, 64.0, 16.0, 4.0], [4.0, 8.0, 128.0, 32.0], [2, 4.0, 32.0, 4.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 28 finished with score 640.0, result : lose board : [[2, 8.0, 16.0, 2.0], [8.0, 16.0, 64.0, 4.0], [4.0, 32.0, 8.0, 2], [2.0, 4.0, 16.0, 8.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 29 finished with score 700.0, result : lose board : [[4.0, 8.0, 4.0, 2], [8.0, 32.0, 8.0, 32.0], [4.0, 16.0, 64.0, 4.0], [2.0, 4.0, 16.0, 2.0]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 30 finished with score 1004.0, result : lose board : [[  4. 128.   4.   2.]\n",
            " [ 16.   8.  32.   8.]\n",
            " [  4.  16.   4.   2.]\n",
            " [  2.   8.   2.   8.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 31 finished with score 644.0, result : lose board : [[ 2.  4.  2.  4.]\n",
            " [ 8.  2. 64.  8.]\n",
            " [16. 32.  4. 32.]\n",
            " [ 2.  4.  8.  2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 32 finished with score 1388.0, result : lose board : [[2.0, 4.0, 64.0, 4.0], [4.0, 8.0, 16.0, 2.0], [32.0, 128.0, 2.0, 4.0], [4, 8.0, 32.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 33 finished with score 876.0, result : lose board : [[  2.   4.   2.   4.]\n",
            " [  8. 128.  16.   2.]\n",
            " [  2.   4.   8.  16.]\n",
            " [  4.   2.   4.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 34 finished with score 748.0, result : lose board : [[4.0, 16.0, 32.0, 4.0], [16.0, 2.0, 64.0, 2.0], [8.0, 16.0, 32.0, 16.0], [2, 4, 8.0, 2]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 35 finished with score 2128.0, result : lose board : [[  2.   8.  32.   2.]\n",
            " [  4.  32.  16.   8.]\n",
            " [  2.   8. 256.   2.]\n",
            " [  4.   2.  16.   4.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 36 finished with score 1360.0, result : lose board : [[  4.  32.   8.   2.]\n",
            " [ 16.   4.  64.  16.]\n",
            " [  4.   8. 128.   4.]\n",
            " [  2.  16.   4.   2.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 37 finished with score 724.0, result : lose board : [[2.0, 16.0, 2.0, 8.0], [64.0, 4.0, 32.0, 4.0], [4.0, 16.0, 8.0, 32.0], [2, 4, 16.0, 4]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 38 finished with score 600.0, result : lose board : [[ 4.  2. 16.  2.]\n",
            " [ 8. 64.  2.  4.]\n",
            " [ 2.  4. 32. 16.]\n",
            " [ 4. 16.  2.  4.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 39 finished with score 612.0, result : lose board : [[4.0, 2.0, 16.0, 2.0], [16.0, 32.0, 8.0, 4.0], [4.0, 16.0, 4.0, 64.0], [2.0, 8.0, 2.0, 4]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 40 finished with score 664.0, result : lose board : [[ 2.  4.  8.  2.]\n",
            " [ 4. 32. 64.  4.]\n",
            " [ 8.  4. 32. 16.]\n",
            " [ 4.  2.  8.  4.]], epsilon  : 0.8955223880597016, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 41 finished with score 2324.0, result : lose board : [[2, 8.0, 2.0, 4.0], [4.0, 64.0, 8.0, 2.0], [8.0, 16.0, 256.0, 32.0], [4.0, 8.0, 16.0, 2.0]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 42 finished with score 1480.0, result : lose board : [[  2.   4.  32.   8.]\n",
            " [  4.   8.  16.  64.]\n",
            " [128.  16.  32.   2.]\n",
            " [  4.   2.  16.   4.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 43 finished with score 1064.0, result : lose board : [[2, 4, 16.0, 2], [8.0, 16.0, 2.0, 8.0], [4.0, 8.0, 128.0, 4.0], [32.0, 16.0, 8.0, 2]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 44 finished with score 712.0, result : lose board : [[ 2.  8. 32.  2.]\n",
            " [ 4. 16.  8.  4.]\n",
            " [16. 64.  2.  8.]\n",
            " [ 2. 32.  8.  4.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 45 finished with score 1044.0, result : lose board : [[ 2. 32.  4. 64.]\n",
            " [ 8.  2. 16.  2.]\n",
            " [16. 64. 32.  8.]\n",
            " [ 2.  4. 16.  2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 46 finished with score 644.0, result : lose board : [[2, 4.0, 32.0, 2.0], [8.0, 64.0, 2.0, 8.0], [16.0, 4.0, 8.0, 4.0], [4.0, 2.0, 32.0, 2.0]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 47 finished with score 984.0, result : lose board : [[2.0, 32.0, 4.0, 16.0], [4.0, 128.0, 8.0, 4.0], [16.0, 2.0, 4.0, 2.0], [2, 8.0, 2.0, 4.0]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 48 finished with score 1432.0, result : lose board : [[  2.   8.   4.   2.]\n",
            " [  4.  64. 128.  32.]\n",
            " [  8.  16.  32.   2.]\n",
            " [  4.   2.  16.   8.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 49 finished with score 592.0, result : lose board : [[ 2. 32.  8.  2.]\n",
            " [ 4. 64. 16.  8.]\n",
            " [ 8.  4.  2. 16.]\n",
            " [ 4.  2.  8.  2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 50 finished with score 672.0, result : lose board : [[ 2. 16.  4.  2.]\n",
            " [ 4.  2. 32.  4.]\n",
            " [32. 64. 16.  8.]\n",
            " [ 2.  8.  2.  4.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Mini-Batch - 0 Back-Prop : 0, Loss : 0.7204155921936035\n",
            "Mini-Batch - 1 Back-Prop : 0, Loss : 0.8195531964302063\n",
            "Mini-Batch - 2 Back-Prop : 0, Loss : 0.7844467163085938\n",
            "Mini-Batch - 3 Back-Prop : 0, Loss : 0.8581467866897583\n",
            "Mini-Batch - 4 Back-Prop : 0, Loss : 0.8826887011528015\n",
            "Mini-Batch - 5 Back-Prop : 0, Loss : 0.8840147256851196\n",
            "Mini-Batch - 6 Back-Prop : 0, Loss : 0.7984393835067749\n",
            "Mini-Batch - 7 Back-Prop : 0, Loss : 0.696048378944397\n",
            "Mini-Batch - 8 Back-Prop : 0, Loss : 0.7568522691726685\n",
            "Mini-Batch - 9 Back-Prop : 0, Loss : 0.7228901386260986\n",
            "Mini-Batch - 10 Back-Prop : 0, Loss : 0.8020823001861572\n",
            "Episode 51 finished with score 1036.0, result : lose board : [[2.0, 8.0, 2.0, 4.0], [4.0, 128.0, 8.0, 2.0], [16.0, 8.0, 32.0, 4.0], [4.0, 16.0, 8.0, 2]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 52 finished with score 1220.0, result : lose board : [[4.0, 2.0, 8.0, 4.0], [2.0, 16.0, 128.0, 16.0], [8.0, 4.0, 64.0, 4.0], [2.0, 16.0, 4.0, 2]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 53 finished with score 1464.0, result : lose board : [[  2.   8.  16.   4.]\n",
            " [  8. 128.   4.  32.]\n",
            " [ 32.   2.  64.   4.]\n",
            " [  2.   8.  16.   2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 54 finished with score 1292.0, result : lose board : [[2.0, 4.0, 16.0, 4.0], [4.0, 16.0, 128.0, 8.0], [8.0, 64.0, 16.0, 2.0], [2, 16.0, 8, 4]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 55 finished with score 316.0, result : lose board : [[ 4.  2.  4.  2.]\n",
            " [16.  8. 32.  8.]\n",
            " [ 8.  4.  8. 16.]\n",
            " [ 2. 16.  4.  2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 56 finished with score 560.0, result : lose board : [[4.0, 2.0, 16.0, 2.0], [2.0, 4.0, 2.0, 16.0], [4.0, 64.0, 32.0, 8.0], [2, 4.0, 8.0, 4]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 57 finished with score 772.0, result : lose board : [[ 4.  8.  4.  2.]\n",
            " [ 2. 64.  8.  4.]\n",
            " [ 4. 16. 64. 16.]\n",
            " [ 2.  8.  4.  2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 58 finished with score 624.0, result : lose board : [[ 4.  2.  4.  2.]\n",
            " [ 8. 64. 32. 16.]\n",
            " [ 4. 16.  2.  8.]\n",
            " [ 2.  8. 16.  2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 59 finished with score 1448.0, result : lose board : [[  2.  32.   2.   8.]\n",
            " [  8.  16.   4.   2.]\n",
            " [  4.  32. 128.   4.]\n",
            " [  2.  64.  16.   2.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 60 finished with score 928.0, result : lose board : [[2.0, 16.0, 8.0, 2], [8.0, 4.0, 128.0, 4.0], [4.0, 16.0, 8.0, 16.0], [2.0, 4.0, 2.0, 4.0]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 61 finished with score 1332.0, result : lose board : [[  2.  32.   4.   2.]\n",
            " [  4. 128.   8.  16.]\n",
            " [ 16.  64.   4.   2.]\n",
            " [  8.   2.   8.   4.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 62 finished with score 712.0, result : lose board : [[ 2.  4.  2.  4.]\n",
            " [ 8. 32. 64.  2.]\n",
            " [ 4. 16. 32.  8.]\n",
            " [ 2.  8. 16.  4.]], epsilon  : 0.8910670527957231, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 63 finished with score 2336.0, result : lose board : [[  2.  16.   4.   2.]\n",
            " [  4.  32.   2.   8.]\n",
            " [ 16.  64.  16.   4.]\n",
            " [  4.   8. 256.   2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 64 finished with score 760.0, result : lose board : [[ 2.  4. 16.  2.]\n",
            " [ 4. 32.  2.  8.]\n",
            " [16.  8. 64. 32.]\n",
            " [ 8. 16.  8.  2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 65 finished with score 1072.0, result : lose board : [[  2.   4.   8.   2.]\n",
            " [ 32.  16. 128.   4.]\n",
            " [  2.   8.   4.  16.]\n",
            " [ 16.   2.   8.   4.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 66 finished with score 704.0, result : lose board : [[ 2. 32.  2.  8.]\n",
            " [ 4.  2. 16. 32.]\n",
            " [16.  8. 64.  2.]\n",
            " [ 4.  2.  4.  8.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 67 finished with score 1732.0, result : lose board : [[  2.   4.   8.   2.]\n",
            " [  4.  16.  32.   8.]\n",
            " [  8. 128.   4.   2.]\n",
            " [  4.   8. 128.   4.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 68 finished with score 2332.0, result : lose board : [[2.0, 4.0, 16.0, 2.0], [4.0, 16.0, 256.0, 4.0], [64.0, 8.0, 16.0, 2], [2.0, 4.0, 32.0, 8.0]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 69 finished with score 904.0, result : lose board : [[2, 4.0, 8.0, 2], [8.0, 16.0, 32.0, 8.0], [4.0, 64.0, 16.0, 2.0], [2.0, 16.0, 4.0, 64.0]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 70 finished with score 2400.0, result : lose board : [[  2.   8.   4.   2.]\n",
            " [  4.  64.  32.   8.]\n",
            " [ 32. 256.  16.   4.]\n",
            " [  8.   2.   4.   8.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 71 finished with score 2356.0, result : lose board : [[4.0, 2.0, 32.0, 2.0], [64.0, 4.0, 16.0, 8.0], [4.0, 16.0, 256.0, 16.0], [2, 4.0, 8.0, 4]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 72 finished with score 1060.0, result : lose board : [[  2.  16.   2.   8.]\n",
            " [  4.   2. 128.   2.]\n",
            " [ 16.  32.  16.   8.]\n",
            " [  4.   2.   8.   2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 73 finished with score 1764.0, result : lose board : [[  2.   4.   2.  32.]\n",
            " [  4.   8. 128.   8.]\n",
            " [  8.   2.  32.   4.]\n",
            " [  2. 128.   4.   2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 74 finished with score 1376.0, result : lose board : [[2, 4.0, 2.0, 4.0], [16.0, 128.0, 64.0, 8.0], [32.0, 2.0, 32.0, 2.0], [2.0, 4.0, 8.0, 4.0]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 75 finished with score 2096.0, result : lose board : [[  2.  16.   8.   2.]\n",
            " [  4.  32.  16.   4.]\n",
            " [  2.  16.   4. 256.]\n",
            " [  4.   2.  16.   4.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 76 finished with score 756.0, result : lose board : [[ 8.  2.  4.  8.]\n",
            " [16. 32. 16. 32.]\n",
            " [ 4.  2. 64.  8.]\n",
            " [ 2.  8. 16.  2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 77 finished with score 2040.0, result : lose board : [[  4.   2.   8.   4.]\n",
            " [  2.  32. 128.   8.]\n",
            " [  8.  16.  64.   4.]\n",
            " [  2.   4. 128.   2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 78 finished with score 2252.0, result : lose board : [[4.0, 8.0, 4.0, 2], [2.0, 4.0, 256.0, 32.0], [8.0, 16.0, 32.0, 16.0], [2.0, 32.0, 4.0, 2.0]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 79 finished with score 676.0, result : lose board : [[ 2.  4. 32.  4.]\n",
            " [16. 64.  8.  2.]\n",
            " [ 2.  4. 32. 16.]\n",
            " [ 4.  2.  8.  2.]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 80 finished with score 600.0, result : lose board : [[2.0, 4.0, 2.0, 8.0], [8.0, 16.0, 64.0, 2.0], [4.0, 2.0, 32.0, 16.0], [2, 16.0, 4.0, 2.0]], epsilon  : 0.886633883378829, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 81 finished with score 1248.0, result : lose board : [[8.0, 4.0, 2.0, 4.0], [2.0, 16.0, 8.0, 16.0], [8.0, 128.0, 64.0, 4.0], [4.0, 8.0, 16.0, 2]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 82 finished with score 1232.0, result : lose board : [[  4.   2.  16.   2.]\n",
            " [ 16.   4. 128.  16.]\n",
            " [  4.  64.   4.   8.]\n",
            " [  2.   8.   2.   4.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 83 finished with score 996.0, result : lose board : [[64.  2.  8.  2.]\n",
            " [ 2.  8.  2.  4.]\n",
            " [ 4. 64.  4.  8.]\n",
            " [ 2. 16. 64.  4.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 84 finished with score 1476, result : lose board : [[  2   4  16   4]\n",
            " [  4   8  64  16]\n",
            " [ 32 128  32   4]\n",
            " [  2   4  16   2]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 85 finished with score 1540.0, result : lose board : [[  2.  64.   8.   2.]\n",
            " [ 16.   8.  16.   4.]\n",
            " [  2.   4.  64.   8.]\n",
            " [  4.  16. 128.   2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 86 finished with score 1020.0, result : lose board : [[  2.   8.   4.   2.]\n",
            " [  4.  16.   2.  16.]\n",
            " [  2. 128.  32.   4.]\n",
            " [  4.   2.  16.   2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 87 finished with score 1168.0, result : lose board : [[64.0, 2.0, 4.0, 8.0], [2.0, 16.0, 8.0, 2.0], [4.0, 128.0, 2.0, 8.0], [2, 4.0, 8.0, 4]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 88 finished with score 740.0, result : lose board : [[ 4. 16.  4.  2.]\n",
            " [32. 64. 16.  4.]\n",
            " [ 2. 16. 32.  8.]\n",
            " [ 8.  2.  8.  2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 89 finished with score 2264.0, result : lose board : [[  2.   4.   8.   4.]\n",
            " [  4.   8.  32.   8.]\n",
            " [  8. 256.  16.  64.]\n",
            " [  4.   2.   4.   2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 90 finished with score 628.0, result : lose board : [[ 2.  8.  4.  2.]\n",
            " [ 4. 16. 64.  4.]\n",
            " [16.  4. 32.  8.]\n",
            " [ 4. 16.  8.  4.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 91 finished with score 1764.0, result : lose board : [[  2.   8.   2.   8.]\n",
            " [  4.  16.  32. 128.]\n",
            " [ 16. 128.   8.   4.]\n",
            " [  2.   8.   4.   2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 92 finished with score 1464.0, result : lose board : [[8.0, 2.0, 4.0, 8.0], [2.0, 128.0, 8.0, 2.0], [8.0, 4.0, 16.0, 64.0], [2.0, 64.0, 4.0, 2]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 93 finished with score 1072.0, result : lose board : [[2.0, 8.0, 16.0, 4.0], [8, 16.0, 128.0, 2], [2.0, 8.0, 16.0, 8.0], [4.0, 32.0, 4.0, 2.0]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 94 finished with score 528.0, result : lose board : [[2, 8.0, 32.0, 4.0], [8.0, 2.0, 64.0, 2.0], [4.0, 8.0, 2.0, 4.0], [2.0, 16.0, 4.0, 2.0]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 95 finished with score 1528.0, result : lose board : [[2, 8.0, 4.0, 2], [8.0, 32.0, 128.0, 4.0], [4.0, 64.0, 32.0, 8.0], [2.0, 32.0, 16.0, 2.0]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 96 finished with score 380.0, result : lose board : [[2.0, 4.0, 32.0, 2], [4.0, 16.0, 8.0, 4.0], [8.0, 32.0, 4.0, 2.0], [2.0, 4.0, 2.0, 16.0]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Mini-Batch - 0 Back-Prop : 1, Loss : 0.7242429256439209\n",
            "Mini-Batch - 1 Back-Prop : 1, Loss : 0.7930921316146851\n",
            "Mini-Batch - 2 Back-Prop : 1, Loss : 0.849403977394104\n",
            "Mini-Batch - 3 Back-Prop : 1, Loss : 0.7240464687347412\n",
            "Mini-Batch - 4 Back-Prop : 1, Loss : 0.7876849174499512\n",
            "Mini-Batch - 5 Back-Prop : 1, Loss : 0.7744145393371582\n",
            "Mini-Batch - 6 Back-Prop : 1, Loss : 0.7585608959197998\n",
            "Mini-Batch - 7 Back-Prop : 1, Loss : 0.8788710236549377\n",
            "Mini-Batch - 8 Back-Prop : 1, Loss : 0.8020966053009033\n",
            "Mini-Batch - 9 Back-Prop : 1, Loss : 0.8854212760925293\n",
            "Mini-Batch - 10 Back-Prop : 1, Loss : 0.8781514763832092\n",
            "Episode 97 finished with score 636.0, result : lose board : [[2.0, 16.0, 4.0, 2.0], [4.0, 32.0, 2.0, 16.0], [8.0, 64.0, 16.0, 4.0], [2, 16.0, 4.0, 2]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 98 finished with score 696.0, result : lose board : [[ 8.  4.  8.  4.]\n",
            " [ 4.  8. 32.  8.]\n",
            " [16. 64.  8.  2.]\n",
            " [ 2.  8. 32.  8.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Episode 99 finished with score 1292.0, result : lose board : [[  2.   8.  16.   4.]\n",
            " [  8.  16. 128.   8.]\n",
            " [  2.  64.  16.   4.]\n",
            " [ 16.   2.   8.   2.]], epsilon  : 0.8822227695311732, learning rate : 0.0005000000237487257 \n",
            "\n",
            "Maximum Score : 2480.0 ,Episode : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI11Ts6B0N79"
      },
      "source": [
        "**Store the training weight data in a file**\n",
        "\n",
        "Make sure to save a path file in your computer and this is the file that you will use in the gamplay to call the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6NCQnWXoHep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a1b87981-478a-4bcb-ee14-a14def1c998b"
      },
      "source": [
        "path = r'C:\\Users\\brandono\\2048-deep-reinforcement-learning\\Weights'\n",
        "weights = ['conv1_layer1_weights','conv1_layer2_weights','conv2_layer1_weights','conv2_layer2_weights','fc_layer1_weights','fc_layer1_biases','fc_layer2_weights','fc_layer2_biases']\n",
        "for w in weights:\n",
        "    flatten = final_parameters[w].reshape(-1,1)\n",
        "    files = open(path + '\\\\' + w +'.csv','w')\n",
        "    files.write('Sno,Weight\\n')\n",
        "    for i in range(flatten.shape[0]):\n",
        "        files.write(str(i) +',' +str(flatten[i][0])+'\\n') \n",
        "    files.close()\n",
        "    print(w + \" written!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1_layer1_weights written!\n",
            "conv1_layer2_weights written!\n",
            "conv2_layer1_weights written!\n",
            "conv2_layer2_weights written!\n",
            "fc_layer1_weights written!\n",
            "fc_layer1_biases written!\n",
            "fc_layer2_weights written!\n",
            "fc_layer2_biases written!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LhDmdu9SGWJ"
      },
      "source": [
        "Works Cited and Additional Resources\n",
        "\n",
        "Amar, J., &amp; Dedieu, A. (n.d.). Deep Reinforcement Learning for 2048. Retrieved August 6, 2020, from http://www.mit.edu/~amarj/files/2048.pdf\n",
        "\n",
        "\n",
        "Science Institute, W. (n.d.). 2Deep Learning and FeedForward Networks (Simply Explained) .pptx. Retrieved August 07, 2020, from https://docs.google.com/presentation/d/e/2PACX-1vTLeO6zMBVQYICIoQLvVEf2IPJJ31H4vc15nCbuTeSDYib6bLpRCxo5I0iPD8yA5A/pub?start=false\n",
        "\n",
        "\n",
        "Adaptation for code mainly from: <Navjinder Virdee > (<Jun 10, 2018>) <2048-deep-reinforcement-learning> (<Old-Code>) [<Conv-2048.ipynb>]. https://github.com/navjindervirdee/2048-deep-reinforcement-learning/blob/master/Code/Old%20Code/Conv-2048.ipynb.\n",
        "\n",
        "\n",
        "Tjwei (Director). (n.d.). [Video file]. Retrieved August 09, 2020, from https://github.com/tjwei/2048-NN\n",
        "\n",
        "\n"
      ]
    }
  ]
}